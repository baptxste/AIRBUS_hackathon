{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from typing import Tuple, Optional, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from env import MazeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open config file\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(\n",
    "    size=20,\n",
    "    walls_proportion=0.2,\n",
    "    num_dynamic_obstacles=1,\n",
    "    num_agents=1,\n",
    "    communication_range=config[\"communication_range\"],\n",
    "    max_lidar_dist_main=config[\"max_lidar_dist_main\"],\n",
    "    max_lidar_dist_second=config[\"max_lidar_dist_second\"],\n",
    "    max_episode_steps=config[\"max_episode_steps\"],\n",
    "    render_mode=config[\"render_mode\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q Network to model the Q function.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A star algorithm\n",
    "class AStarPathfinder:\n",
    "    def __init__(self, grid_size, obstacles):\n",
    "        self.grid_size = grid_size\n",
    "        self.obstacles = obstacles # list of (x, y) tuples\n",
    "\n",
    "    def heuristic(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def get_neighbors(self, node):\n",
    "        x, y = node\n",
    "        neighbors = [(x+1, y), (x-1, y), (x, y+1), (x, y-1)]\n",
    "        return [n for n in neighbors if 0 <= n[0] < self.grid_size[0] and 0 <= n[1] < self.grid_size[1] and n not in self.obstacles]\n",
    "\n",
    "    def find_path(self, start, goal):\n",
    "        open_list = []\n",
    "        heapq.heappush(open_list, (0, start))\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        f_score = {start: self.heuristic(start, goal)}\n",
    "\n",
    "        while open_list:\n",
    "            _, current = heapq.heappop(open_list)\n",
    "            if current == goal:\n",
    "                path = []\n",
    "                while current in came_from:\n",
    "                    path.append(current)\n",
    "                    current = came_from[current]\n",
    "                path.reverse()\n",
    "                return path\n",
    "\n",
    "            for neighbor in self.get_neighbors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g_score\n",
    "                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_list, (f_score[neighbor], neighbor))\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgents():\n",
    "    def __init__(self, state_dim: int, action_dim: int, device : str, lr: float):\n",
    "        self.device = torch.device(device)        \n",
    "        # state dim = 11 pour preprocess_states\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # define the Networks\n",
    "        self.q_network = DQN.DQN(input_dim=state_dim, output_dim=action_dim).to(self.device)\n",
    "        self.target_network = DQN.DQN(input_dim=state_dim, output_dim=action_dim).to(self.device)\n",
    "        \n",
    "        # define the optimizer\n",
    "        self.optimizers = torch.optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "        \n",
    "        # define the hyperparameters\n",
    "        self.batch_size = 128\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.gamma = 0.99\n",
    "        self.TAU = 0.005\n",
    "        \n",
    "        # define the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(maxlen=10000)\n",
    "    \n",
    "    def process_states(self, state: list) :\n",
    "        final = []\n",
    "        if state[3]==0: # état du drone\n",
    "            main_state = np.concatenate((state[:3], state[6:12]))\n",
    "            goal = (state[4], state[5])\n",
    "            main_state[0] = main_state[0] - goal[0]\n",
    "            main_state[1] = main_state[1] - goal[1]\n",
    "            other_state = state[12:]\n",
    "            num_drone = int(len(other_state)/10)\n",
    "            avg_position = [(state[0], state[1])]\n",
    "\n",
    "            for i in range(num_drone): # on récupère la postion de chaque drone pour calculer la position moyenne de l'essaim \n",
    "                if other_state[10*i+3] == 0 :\n",
    "                    avg_position.append(( other_state[10*i+0],  other_state[10*i+1]))\n",
    "            mean_position = np.round(np.mean(avg_position, axis=0)).astype(int) - goal\n",
    "            \n",
    "            # l'état final contient la position du drone, du goal et les infos lidar et la position moyenne de l'essaim\n",
    "            final.append(np.concatenate((main_state, mean_position)).astype(int))\n",
    "        if state[3]!=0: # état du drone\n",
    "            final.append(np.full((11), -1))\n",
    "        return final\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = self.process_states(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.choice(np.arange(self.action_dim))\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "            q_values = torch.softmax(self.q_network(state_tensor),dim=-1)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            action = max(0, min(action, self.action_dim - 1))\n",
    "            return action\n",
    "\n",
    "    def update_target_network(self):\n",
    "        target_net_state_dict = self.target_network.state_dict()\n",
    "        policy_net_state_dict = self.q_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "        self.target_network.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "    def update_policy(self):\n",
    "        if self.replay_buffer.__len__() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*batch)\n",
    "        \n",
    "        #clean states\n",
    "        batch_dim = len(batch_states)\n",
    "        clean_batch_states = []\n",
    "        clean_batch_next_states = []\n",
    "        for i in range(batch_dim):\n",
    "            clean_batch_states.append(self.process_states(batch_states[i]))\n",
    "            clean_batch_next_states.append(self.process_states(batch_next_states[i]))\n",
    "        batch_states = np.array(clean_batch_states)\n",
    "        batch_next_states = np.array(clean_batch_next_states)\n",
    "        \n",
    "        #convert to tensor\n",
    "        states = torch.FloatTensor(batch_states).squeeze(1).to(self.device)\n",
    "        actions = torch.LongTensor(batch_actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(batch_rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(batch_next_states).squeeze(1).to(self.device)\n",
    "        dones = torch.FloatTensor(batch_dones).to(self.device)\n",
    "        \n",
    "        # compute actual q values\n",
    "        q_values = self.q_network(states)\n",
    "        # Retrieve the q values for the actions that were taken\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # compute target q values\n",
    "        with torch.no_grad():\n",
    "            target_q_values = self.target_network(next_states)\n",
    "            target_q_values = rewards + self.gamma * target_q_values.max(dim=1).values * (1 - dones)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_value_(self.q_network.parameters(), 100)\n",
    "        self.optimizers.step()\n",
    "        \n",
    "        # update epsilon\n",
    "        #self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

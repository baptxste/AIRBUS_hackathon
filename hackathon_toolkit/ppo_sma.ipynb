{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30578,"status":"ok","timestamp":1741509333207,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"BmTVAOXtgdpw","outputId":"1588e41b-caa7-4b61-b1a9-cedfd18f8b4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":848,"status":"ok","timestamp":1741509335139,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"yYPAmVTNgxjH","outputId":"37ea6421-e739-4d52-922f-42791cb733b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ECM/IAM/Hackathon/Solution\n"]}],"source":["%cd /content/drive/MyDrive/ECM/IAM/Hackathon/Solution/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1741509336063,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"TcymuniThFzP","outputId":"7396e52e-4cde-42db-e07e-a6fcb3f4a573"},"outputs":[{"output_type":"stream","name":"stdout","text":["actor_best.pth   config.json      env.py         process_state.py  simulate.py\n","actor.pth        critic_best.pth  \u001b[0m\u001b[01;34meval_configs\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\n","all_results.csv  critic.pth       ppo_sma.ipynb  reward.py\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5546,"status":"ok","timestamp":1741509342508,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"WBjkMw_ngIML"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","from torch.distributions import MultivariateNormal\n","import numpy as np\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import json\n","from typing import Tuple, Optional, Dict\n","import torch\n","import gymnasium as gym\n","\n","\n","from env import MazeEnv"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81,"status":"ok","timestamp":1741509342591,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"aP9kf_T6iDya","outputId":"bf150d61-971f-4d09-a7ad-994943af62e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":5}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1741509342617,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"w6ES6x0ogIMQ"},"outputs":[],"source":["class Actor(nn.Module):\n","    def __init__(self, state_size, action_size, hidden_size=32, low_policy_weights_init=True):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(state_size, 64), nn.ReLU(),\n","            nn.Linear(64, 64), nn.ReLU(),\n","            nn.Linear(64, action_size)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, state_size, hidden_size=32):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(state_size, 64), nn.ReLU(),\n","            nn.Linear(64, 64), nn.ReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1741514516201,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"wDRp6fLQgIMQ"},"outputs":[],"source":["import torch.distributions as dist\n","from process_state import StateNormalizer\n","\n","class MAPPOAgent:\n","    def __init__(self, state_size, action_size, n_agents, lr=1e-4, grid_size = 30, max_lidar_range = 8):\n","        self.n_agents = n_agents\n","        self.actor = Actor(state_size, action_size).to(device)\n","        self.critic = Critic(state_size).to(device)\n","        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr)\n","        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr)\n","        self.normalizer = StateNormalizer(grid_size, max_lidar_range)\n","\n","    def select_actions(self, states):\n","        \"\"\" Sélectionne les actions pour tous les agents en parallèle \"\"\"\n","        states = np.array([self.normalizer.normalize_agent_state(s) for s in states])\n","        states = torch.tensor(states, dtype=torch.float32, device=device)  # (n_agents, obs_dim)\n","        probs = self.actor(states)  # (n_agents, act_dim)\n","        distribution = dist.Categorical(logits=probs)\n","        actions = distribution.sample()  # (n_agents,)\n","        log_probs = distribution.log_prob(actions)  # (n_agents,)\n","        return actions.cpu().detach().numpy(), log_probs.cpu().detach().numpy()  # Retourne toutes les actions et log_probs\n","\n","    def compute_loss(self, states, actions, log_probs_old, rewards, dones, gamma=0.99, clip_eps=0.2):\n","        states = np.array([self.normalizer.normalize_agent_state(s) for s in states])\n","        states = torch.tensor(states, dtype=torch.float32, device=device)\n","        actions = torch.tensor(actions, dtype=torch.long, device=device)\n","        log_probs_old = torch.tensor(log_probs_old, dtype=torch.float32, device=device)\n","        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n","\n","        # Compute advantage\n","        values = self.critic(states).squeeze()\n","        dones = torch.tensor(dones, dtype=torch.float32,device=device)\n","        returns = rewards + gamma * values * (1 - dones)\n","        advantage = returns - values.detach()\n","        #normalize advantage\n","        #advantage = (advantage - advantage.mean()) / (advantage.std()+1e-5)\n","        # Compute new log_probs\n","        probs = self.actor(states)\n","        dist_new = dist.Categorical(logits=probs)\n","        log_probs_new = dist_new.log_prob(actions)\n","\n","        # PPO Clip loss\n","        ratio = torch.exp(log_probs_new - log_probs_old)\n","        clipped_ratio = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps)\n","\n","        # Calcul de l'entropie pour encourager l'exploration\n","        entropy = dist_new.entropy().mean()  # Mesure d'incertitude de la politique\n","        entropy_bonus = 0.01 * entropy  # Poids ajustable de l'entropie\n","\n","        actor_loss = -torch.min(ratio * advantage, clipped_ratio * advantage).mean() - entropy_bonus\n","\n","        # Critic loss (MSE loss)\n","        noise = torch.randn_like(returns) * 1 # Petit bruit aléatoire\n","        returns_noisy = returns + noise\n","        critic_loss = F.mse_loss(values, returns_noisy)\n","\n","        return actor_loss, critic_loss\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1741514450789,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"ljMLAdXygIMR"},"outputs":[],"source":["from collections import deque\n","class ReplayBuffer:\n","    def __init__(self):\n","        self.memory = deque(maxlen=100000)\n","\n","    def store(self, trajectory):\n","        self.memory.append(trajectory)\n","\n","    def get_data(self):\n","        return self.memory\n"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1741514563315,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"},"user_tz":-60},"id":"i314dY8BgIMR"},"outputs":[],"source":["def simulation_config(config_path: str, new_agent: bool = True):\n","    \"\"\"\n","    Configure the environment and optionally an agent using a JSON configuration file.\n","\n","    Args:\n","        config_path (str): Path to the configuration JSON file.\n","        new_agent (bool): Whether to initialize the agent. Defaults to True.\n","\n","    Returns:\n","        Tuple[MazeEnv, Optional[MyAgent], Dict]: Configured environment, agent (if new), and the configuration dictionary.\n","    \"\"\"\n","\n","    # Read config\n","    with open(config_path, 'r') as config_file:\n","        config = json.load(config_file)\n","\n","    # Env configuration\n","    env = MazeEnv(\n","        size=config.get('grid_size'),                               # Grid size\n","        walls_proportion=config.get('walls_proportion'),            # Walls proportion in the grid\n","        num_dynamic_obstacles=config.get('num_dynamic_obstacles'),  # Number of dynamic obstacles\n","        num_agents=config.get('num_agents'),                        # Number of agents\n","        communication_range=config.get('communication_range'),      # Maximum distance for agent communications\n","        max_lidar_dist_main=config.get('max_lidar_dist_main'),      # Maximum distance for main LIDAR scan\n","        max_lidar_dist_second=config.get('max_lidar_dist_second'),  # Maximum distance for secondary LIDAR scan\n","        max_episode_steps=config.get('max_episode_steps'),          # Number of steps before episode termination\n","        render_mode=None,\n","        seed=config.get('seed', None)                               # Seed for reproducibility\n","    )\n","    num_agents = env.num_agents\n","    # Agent configuration\n","    agents = MAPPOAgent(state_size=98,action_size=env.action_space.n,n_agents=num_agents, lr=1e-5) if new_agent else None\n","\n","    return env, agents, config"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-8efbr0QgIMS","executionInfo":{"status":"error","timestamp":1741515203317,"user_tz":-60,"elapsed":638725,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"}},"outputId":"fb36227a-479e-43af-d242-40d75ee63aca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 0, Reward: 1973.42, Evacuated: 3, Deactivated: 1, Actor Loss: -1.24, Critic Loss: 153.82\n","Episode 1, Reward: 1138.13, Evacuated: 1, Deactivated: 3, Actor Loss: -1.13, Critic Loss: 136.66\n","Episode 2, Reward: 976.31, Evacuated: 0, Deactivated: 4, Actor Loss: -0.85, Critic Loss: 121.67\n","Episode 3, Reward: 859.87, Evacuated: 1, Deactivated: 3, Actor Loss: -0.99, Critic Loss: 125.19\n","Episode 4, Reward: 1319.36, Evacuated: 1, Deactivated: 3, Actor Loss: -0.79, Critic Loss: 125.37\n","Episode 5, Reward: 1288.30, Evacuated: 1, Deactivated: 3, Actor Loss: -0.76, Critic Loss: 125.39\n","Episode 6, Reward: 1812.53, Evacuated: 2, Deactivated: 2, Actor Loss: -0.74, Critic Loss: 125.25\n","Episode 7, Reward: 1767.10, Evacuated: 0, Deactivated: 4, Actor Loss: -0.48, Critic Loss: 131.59\n","Episode 8, Reward: 2058.53, Evacuated: 3, Deactivated: 1, Actor Loss: -0.62, Critic Loss: 134.30\n","Episode 9, Reward: 717.73, Evacuated: 1, Deactivated: 3, Actor Loss: -0.74, Critic Loss: 133.85\n","Episode 10, Reward: 1368.30, Evacuated: 1, Deactivated: 3, Actor Loss: -0.79, Critic Loss: 131.89\n","Episode 11, Reward: 2000.28, Evacuated: 3, Deactivated: 1, Actor Loss: -0.81, Critic Loss: 133.95\n","Episode 12, Reward: 2604.76, Evacuated: 4, Deactivated: 0, Actor Loss: -0.90, Critic Loss: 134.86\n","Episode 13, Reward: 1588.32, Evacuated: 2, Deactivated: 2, Actor Loss: -0.91, Critic Loss: 135.47\n","Episode 14, Reward: 2246.66, Evacuated: 3, Deactivated: 1, Actor Loss: -0.87, Critic Loss: 136.43\n","Episode 15, Reward: 1237.28, Evacuated: 1, Deactivated: 3, Actor Loss: -0.85, Critic Loss: 135.13\n","Episode 16, Reward: 2336.81, Evacuated: 3, Deactivated: 1, Actor Loss: -0.92, Critic Loss: 136.06\n","Episode 17, Reward: 819.83, Evacuated: 1, Deactivated: 3, Actor Loss: -0.93, Critic Loss: 136.40\n","Episode 18, Reward: 160.44, Evacuated: 0, Deactivated: 4, Actor Loss: -0.86, Critic Loss: 138.07\n","Episode 19, Reward: 1626.66, Evacuated: 1, Deactivated: 3, Actor Loss: -0.87, Critic Loss: 137.25\n","Episode 20, Reward: 1747.75, Evacuated: 1, Deactivated: 3, Actor Loss: -0.87, Critic Loss: 136.58\n","Episode 21, Reward: 2162.52, Evacuated: 3, Deactivated: 1, Actor Loss: -0.85, Critic Loss: 136.71\n","Episode 22, Reward: 1936.37, Evacuated: 3, Deactivated: 1, Actor Loss: -0.83, Critic Loss: 137.27\n","Episode 23, Reward: 2033.40, Evacuated: 3, Deactivated: 1, Actor Loss: -0.81, Critic Loss: 137.24\n","Episode 24, Reward: 928.91, Evacuated: 1, Deactivated: 3, Actor Loss: -0.84, Critic Loss: 137.19\n","Episode 25, Reward: 899.87, Evacuated: 1, Deactivated: 3, Actor Loss: -0.86, Critic Loss: 137.37\n","Episode 26, Reward: 567.88, Evacuated: 0, Deactivated: 4, Actor Loss: -0.85, Critic Loss: 135.80\n","Episode 27, Reward: 1027.07, Evacuated: 1, Deactivated: 3, Actor Loss: -0.86, Critic Loss: 135.24\n","Episode 28, Reward: 741.02, Evacuated: 0, Deactivated: 4, Actor Loss: -0.35, Critic Loss: 665.51\n","Episode 29, Reward: 2056.49, Evacuated: 3, Deactivated: 1, Actor Loss: -0.42, Critic Loss: 646.83\n","Episode 30, Reward: 946.28, Evacuated: 0, Deactivated: 4, Actor Loss: -0.37, Critic Loss: 634.13\n","Episode 31, Reward: 1883.33, Evacuated: 3, Deactivated: 1, Actor Loss: -0.39, Critic Loss: 617.85\n","Episode 32, Reward: 2255.70, Evacuated: 3, Deactivated: 1, Actor Loss: -0.41, Critic Loss: 601.10\n","Episode 33, Reward: 1768.52, Evacuated: 2, Deactivated: 2, Actor Loss: -0.43, Critic Loss: 586.71\n","Episode 34, Reward: 1836.57, Evacuated: 2, Deactivated: 2, Actor Loss: -0.41, Critic Loss: 572.34\n","Episode 35, Reward: 1908.38, Evacuated: 3, Deactivated: 1, Actor Loss: -0.44, Critic Loss: 560.69\n","Episode 36, Reward: 1552.93, Evacuated: 0, Deactivated: 4, Actor Loss: -0.42, Critic Loss: 548.01\n","Episode 37, Reward: 1478.53, Evacuated: 1, Deactivated: 3, Actor Loss: -0.40, Critic Loss: 536.91\n","Episode 38, Reward: 1937.68, Evacuated: 2, Deactivated: 2, Actor Loss: -0.41, Critic Loss: 524.50\n","Episode 39, Reward: 2068.50, Evacuated: 3, Deactivated: 1, Actor Loss: -0.43, Critic Loss: 516.01\n","Episode 40, Reward: 1956.41, Evacuated: 3, Deactivated: 1, Actor Loss: -0.45, Critic Loss: 505.28\n","Episode 41, Reward: 1347.09, Evacuated: 2, Deactivated: 2, Actor Loss: -0.44, Critic Loss: 496.40\n","Episode 42, Reward: 1439.12, Evacuated: 2, Deactivated: 2, Actor Loss: -0.44, Critic Loss: 488.22\n","Episode 43, Reward: 917.91, Evacuated: 1, Deactivated: 3, Actor Loss: -0.45, Critic Loss: 478.76\n","Episode 44, Reward: 2185.61, Evacuated: 3, Deactivated: 1, Actor Loss: -0.46, Critic Loss: 470.41\n","Episode 45, Reward: 1001.28, Evacuated: 0, Deactivated: 4, Actor Loss: -0.44, Critic Loss: 464.93\n","Episode 46, Reward: 1468.20, Evacuated: 2, Deactivated: 2, Actor Loss: -0.46, Critic Loss: 457.29\n","Episode 47, Reward: 2241.02, Evacuated: 2, Deactivated: 2, Actor Loss: -0.43, Critic Loss: 459.65\n","Episode 48, Reward: 1827.57, Evacuated: 2, Deactivated: 2, Actor Loss: -0.44, Critic Loss: 451.85\n","Episode 49, Reward: 1997.44, Evacuated: 3, Deactivated: 1, Actor Loss: -0.45, Critic Loss: 444.36\n","Episode 50, Reward: 1399.40, Evacuated: 1, Deactivated: 3, Actor Loss: -0.46, Critic Loss: 438.20\n","Episode 51, Reward: 1846.56, Evacuated: 2, Deactivated: 2, Actor Loss: -0.47, Critic Loss: 431.72\n","Episode 52, Reward: 2036.47, Evacuated: 3, Deactivated: 1, Actor Loss: -0.49, Critic Loss: 425.66\n","Episode 53, Reward: 1038.03, Evacuated: 1, Deactivated: 3, Actor Loss: -0.51, Critic Loss: 420.59\n","Episode 54, Reward: 1458.15, Evacuated: 2, Deactivated: 2, Actor Loss: -0.56, Critic Loss: 414.59\n","Episode 55, Reward: 1098.13, Evacuated: 1, Deactivated: 3, Actor Loss: -0.57, Critic Loss: 410.12\n","Episode 56, Reward: 1834.61, Evacuated: 2, Deactivated: 2, Actor Loss: -0.59, Critic Loss: 404.20\n","Episode 57, Reward: 2622.61, Evacuated: 4, Deactivated: 0, Actor Loss: -0.60, Critic Loss: 399.91\n","Episode 58, Reward: 1290.30, Evacuated: 1, Deactivated: 3, Actor Loss: -0.61, Critic Loss: 394.59\n","Episode 59, Reward: 1735.48, Evacuated: 2, Deactivated: 2, Actor Loss: -0.63, Critic Loss: 390.99\n","Episode 60, Reward: 909.94, Evacuated: 1, Deactivated: 3, Actor Loss: -0.64, Critic Loss: 385.99\n","Episode 61, Reward: 1776.53, Evacuated: 2, Deactivated: 2, Actor Loss: -0.64, Critic Loss: 381.77\n","Episode 62, Reward: 1200.23, Evacuated: 1, Deactivated: 3, Actor Loss: -0.64, Critic Loss: 377.05\n","Episode 63, Reward: 1409.43, Evacuated: 1, Deactivated: 3, Actor Loss: -0.62, Critic Loss: 377.53\n","Episode 64, Reward: 1886.64, Evacuated: 2, Deactivated: 2, Actor Loss: -0.62, Critic Loss: 374.33\n","Episode 65, Reward: 2394.75, Evacuated: 3, Deactivated: 1, Actor Loss: -0.63, Critic Loss: 370.79\n","Episode 66, Reward: 2155.90, Evacuated: 2, Deactivated: 2, Actor Loss: -0.63, Critic Loss: 366.11\n","Episode 67, Reward: 439.73, Evacuated: 0, Deactivated: 4, Actor Loss: -0.62, Critic Loss: 364.88\n","Episode 68, Reward: 1779.54, Evacuated: 2, Deactivated: 2, Actor Loss: -0.64, Critic Loss: 362.15\n","Episode 69, Reward: 1777.44, Evacuated: 2, Deactivated: 2, Actor Loss: -0.65, Critic Loss: 357.82\n","Episode 70, Reward: 2006.76, Evacuated: 2, Deactivated: 2, Actor Loss: -0.65, Critic Loss: 354.58\n","Episode 71, Reward: 1338.37, Evacuated: 1, Deactivated: 3, Actor Loss: -0.65, Critic Loss: 352.22\n","Episode 72, Reward: 1528.30, Evacuated: 2, Deactivated: 2, Actor Loss: -0.66, Critic Loss: 348.51\n","Episode 73, Reward: 1259.26, Evacuated: 1, Deactivated: 3, Actor Loss: -0.67, Critic Loss: 345.52\n","Episode 74, Reward: 1007.05, Evacuated: 1, Deactivated: 3, Actor Loss: -0.68, Critic Loss: 342.13\n","Episode 75, Reward: 270.55, Evacuated: 0, Deactivated: 4, Actor Loss: -0.68, Critic Loss: 340.92\n","Episode 76, Reward: 2256.69, Evacuated: 3, Deactivated: 1, Actor Loss: -0.70, Critic Loss: 338.26\n","Episode 77, Reward: 2118.59, Evacuated: 3, Deactivated: 1, Actor Loss: -0.73, Critic Loss: 335.85\n","Episode 78, Reward: 2016.43, Evacuated: 3, Deactivated: 1, Actor Loss: -0.74, Critic Loss: 333.30\n","Episode 79, Reward: 1647.35, Evacuated: 2, Deactivated: 2, Actor Loss: -0.76, Critic Loss: 330.96\n","Episode 80, Reward: 2183.59, Evacuated: 3, Deactivated: 1, Actor Loss: -0.78, Critic Loss: 327.90\n","Episode 81, Reward: 1379.14, Evacuated: 2, Deactivated: 2, Actor Loss: -0.80, Critic Loss: 325.61\n","Episode 82, Reward: 568.87, Evacuated: 0, Deactivated: 4, Actor Loss: -0.80, Critic Loss: 322.99\n","Episode 83, Reward: 2144.88, Evacuated: 2, Deactivated: 2, Actor Loss: -0.80, Critic Loss: 320.76\n","Episode 84, Reward: 947.98, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 318.69\n","Episode 85, Reward: 1259.56, Evacuated: 0, Deactivated: 4, Actor Loss: -0.81, Critic Loss: 315.49\n","Episode 86, Reward: 767.82, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 313.67\n","Episode 87, Reward: 1578.29, Evacuated: 2, Deactivated: 2, Actor Loss: -0.82, Critic Loss: 311.35\n","Episode 88, Reward: 1178.21, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 309.04\n","Episode 89, Reward: 1537.24, Evacuated: 2, Deactivated: 2, Actor Loss: -0.82, Critic Loss: 306.99\n","Episode 90, Reward: 1408.18, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 304.40\n","Episode 91, Reward: 1833.60, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 303.06\n","Episode 92, Reward: 1367.09, Evacuated: 2, Deactivated: 2, Actor Loss: -0.82, Critic Loss: 300.67\n","Episode 93, Reward: 2006.44, Evacuated: 3, Deactivated: 1, Actor Loss: -0.83, Critic Loss: 299.63\n","Episode 94, Reward: 1329.09, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 297.68\n","Episode 95, Reward: 1167.19, Evacuated: 1, Deactivated: 3, Actor Loss: -0.84, Critic Loss: 295.95\n","Episode 96, Reward: 1018.07, Evacuated: 1, Deactivated: 3, Actor Loss: -0.78, Critic Loss: 400.57\n","Episode 97, Reward: 718.72, Evacuated: 1, Deactivated: 3, Actor Loss: -0.78, Critic Loss: 398.07\n","Episode 98, Reward: 1698.69, Evacuated: 1, Deactivated: 3, Actor Loss: -0.78, Critic Loss: 395.33\n","Episode 99, Reward: 1388.38, Evacuated: 1, Deactivated: 3, Actor Loss: -0.78, Critic Loss: 392.02\n","Episode 100, Reward: 1155.22, Evacuated: 1, Deactivated: 3, Actor Loss: -0.80, Critic Loss: 389.23\n","Episode 101, Reward: 1219.22, Evacuated: 1, Deactivated: 3, Actor Loss: -0.80, Critic Loss: 386.28\n","Episode 102, Reward: 1543.61, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 383.95\n","Episode 103, Reward: 520.80, Evacuated: 0, Deactivated: 4, Actor Loss: -0.80, Critic Loss: 382.95\n","Episode 104, Reward: 1666.01, Evacuated: 0, Deactivated: 4, Actor Loss: -0.77, Critic Loss: 404.68\n","Episode 105, Reward: 2266.74, Evacuated: 3, Deactivated: 1, Actor Loss: -0.78, Critic Loss: 402.26\n","Episode 106, Reward: 1688.79, Evacuated: 1, Deactivated: 3, Actor Loss: -0.78, Critic Loss: 398.68\n","Episode 107, Reward: 1002.38, Evacuated: 0, Deactivated: 4, Actor Loss: -0.77, Critic Loss: 396.14\n","Episode 108, Reward: 1397.14, Evacuated: 2, Deactivated: 2, Actor Loss: -0.76, Critic Loss: 393.44\n","Episode 109, Reward: 1847.89, Evacuated: 1, Deactivated: 3, Actor Loss: -0.76, Critic Loss: 390.70\n","Episode 110, Reward: 1108.16, Evacuated: 1, Deactivated: 3, Actor Loss: -0.76, Critic Loss: 388.12\n","Episode 111, Reward: 1308.34, Evacuated: 1, Deactivated: 3, Actor Loss: -0.77, Critic Loss: 385.96\n","Episode 112, Reward: 1307.07, Evacuated: 2, Deactivated: 2, Actor Loss: -0.76, Critic Loss: 383.70\n","Episode 113, Reward: 1048.08, Evacuated: 1, Deactivated: 3, Actor Loss: -0.77, Critic Loss: 380.87\n","Episode 114, Reward: 688.99, Evacuated: 0, Deactivated: 4, Actor Loss: -0.76, Critic Loss: 379.94\n","Episode 115, Reward: 1363.84, Evacuated: 0, Deactivated: 4, Actor Loss: -0.76, Critic Loss: 376.55\n","Episode 116, Reward: 1688.46, Evacuated: 2, Deactivated: 2, Actor Loss: -0.76, Critic Loss: 374.00\n","Episode 117, Reward: 1348.38, Evacuated: 1, Deactivated: 3, Actor Loss: -0.77, Critic Loss: 372.25\n","Episode 118, Reward: 1408.40, Evacuated: 1, Deactivated: 3, Actor Loss: -0.77, Critic Loss: 369.50\n","Episode 119, Reward: 1809.48, Evacuated: 2, Deactivated: 2, Actor Loss: -0.77, Critic Loss: 366.99\n","Episode 120, Reward: 2039.45, Evacuated: 3, Deactivated: 1, Actor Loss: -0.77, Critic Loss: 365.22\n","Episode 121, Reward: 2155.88, Evacuated: 2, Deactivated: 2, Actor Loss: -0.77, Critic Loss: 363.10\n","Episode 122, Reward: 1585.27, Evacuated: 2, Deactivated: 2, Actor Loss: -0.77, Critic Loss: 360.42\n","Episode 123, Reward: 897.89, Evacuated: 1, Deactivated: 3, Actor Loss: -0.78, Critic Loss: 358.80\n","Episode 124, Reward: 460.74, Evacuated: 0, Deactivated: 4, Actor Loss: -0.77, Critic Loss: 357.84\n","Episode 125, Reward: 2165.57, Evacuated: 3, Deactivated: 1, Actor Loss: -0.78, Critic Loss: 355.82\n","Episode 126, Reward: 1934.67, Evacuated: 2, Deactivated: 2, Actor Loss: -0.80, Critic Loss: 354.49\n","Episode 127, Reward: 1317.30, Evacuated: 1, Deactivated: 3, Actor Loss: -0.80, Critic Loss: 351.95\n","Episode 128, Reward: 2156.59, Evacuated: 3, Deactivated: 1, Actor Loss: -0.81, Critic Loss: 350.63\n","Episode 129, Reward: 1505.27, Evacuated: 2, Deactivated: 2, Actor Loss: -0.82, Critic Loss: 348.39\n","Episode 130, Reward: 1729.45, Evacuated: 2, Deactivated: 2, Actor Loss: -0.84, Critic Loss: 346.74\n","Episode 131, Reward: 1628.40, Evacuated: 2, Deactivated: 2, Actor Loss: -0.84, Critic Loss: 344.52\n","Episode 132, Reward: 1105.45, Evacuated: 0, Deactivated: 4, Actor Loss: -0.84, Critic Loss: 342.93\n","Episode 133, Reward: 1584.66, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 347.64\n","Episode 134, Reward: 1179.48, Evacuated: 0, Deactivated: 4, Actor Loss: -0.81, Critic Loss: 345.19\n","Episode 135, Reward: 2345.77, Evacuated: 3, Deactivated: 1, Actor Loss: -0.82, Critic Loss: 343.19\n","Episode 136, Reward: 899.20, Evacuated: 0, Deactivated: 4, Actor Loss: -0.82, Critic Loss: 341.68\n","Episode 137, Reward: 1768.48, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 340.59\n","Episode 138, Reward: 2037.74, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 338.72\n","Episode 139, Reward: 1356.35, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 337.43\n","Episode 140, Reward: 1100.13, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 335.46\n","Episode 141, Reward: 2236.67, Evacuated: 3, Deactivated: 1, Actor Loss: -0.84, Critic Loss: 334.08\n","Episode 142, Reward: 2623.73, Evacuated: 4, Deactivated: 0, Actor Loss: -0.85, Critic Loss: 332.49\n","Episode 143, Reward: 1515.60, Evacuated: 1, Deactivated: 3, Actor Loss: -0.84, Critic Loss: 333.30\n","Episode 144, Reward: 1766.44, Evacuated: 2, Deactivated: 2, Actor Loss: -0.85, Critic Loss: 331.65\n","Episode 145, Reward: 338.64, Evacuated: 0, Deactivated: 4, Actor Loss: -0.84, Critic Loss: 331.88\n","Episode 146, Reward: 1657.70, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 330.03\n","Episode 147, Reward: 2610.60, Evacuated: 4, Deactivated: 0, Actor Loss: -0.84, Critic Loss: 328.37\n","Episode 148, Reward: 1927.69, Evacuated: 2, Deactivated: 2, Actor Loss: -0.84, Critic Loss: 327.13\n","Episode 149, Reward: 697.02, Evacuated: 0, Deactivated: 4, Actor Loss: -0.83, Critic Loss: 325.21\n","Episode 150, Reward: 798.11, Evacuated: 0, Deactivated: 4, Actor Loss: -0.83, Critic Loss: 324.23\n","Episode 151, Reward: 1502.60, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 322.16\n","Episode 152, Reward: 1297.62, Evacuated: 0, Deactivated: 4, Actor Loss: -0.82, Critic Loss: 321.08\n","Episode 153, Reward: 1460.47, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 319.12\n","Episode 154, Reward: 1120.11, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 317.90\n","Episode 155, Reward: 769.07, Evacuated: 0, Deactivated: 4, Actor Loss: -0.81, Critic Loss: 316.41\n","Episode 156, Reward: 877.20, Evacuated: 0, Deactivated: 4, Actor Loss: -0.81, Critic Loss: 314.54\n","Episode 157, Reward: 758.07, Evacuated: 0, Deactivated: 4, Actor Loss: -0.81, Critic Loss: 313.30\n","Episode 158, Reward: 1967.69, Evacuated: 2, Deactivated: 2, Actor Loss: -0.81, Critic Loss: 312.08\n","Episode 159, Reward: 849.84, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 310.72\n","Episode 160, Reward: 1927.67, Evacuated: 2, Deactivated: 2, Actor Loss: -0.82, Critic Loss: 309.51\n","Episode 161, Reward: 1385.73, Evacuated: 0, Deactivated: 4, Actor Loss: -0.82, Critic Loss: 307.86\n","Episode 162, Reward: 2196.67, Evacuated: 3, Deactivated: 1, Actor Loss: -0.83, Critic Loss: 307.02\n","Episode 163, Reward: 1310.41, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 302.66\n","Episode 164, Reward: 1810.59, Evacuated: 2, Deactivated: 2, Actor Loss: -0.82, Critic Loss: 301.73\n","Episode 165, Reward: 2235.64, Evacuated: 3, Deactivated: 1, Actor Loss: -0.83, Critic Loss: 300.35\n","Episode 166, Reward: 1480.49, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 299.60\n","Episode 167, Reward: 1159.18, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 298.37\n","Episode 168, Reward: 1474.55, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 297.04\n","Episode 169, Reward: 738.76, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 295.86\n","Episode 170, Reward: 1694.43, Evacuated: 2, Deactivated: 2, Actor Loss: -0.85, Critic Loss: 294.70\n","Episode 171, Reward: 849.88, Evacuated: 1, Deactivated: 3, Actor Loss: -0.85, Critic Loss: 293.53\n","Episode 172, Reward: 1111.39, Evacuated: 0, Deactivated: 4, Actor Loss: -0.84, Critic Loss: 292.45\n","Episode 173, Reward: 1248.24, Evacuated: 1, Deactivated: 3, Actor Loss: -0.85, Critic Loss: 291.71\n","Episode 174, Reward: 2364.76, Evacuated: 3, Deactivated: 1, Actor Loss: -0.86, Critic Loss: 290.48\n","Episode 175, Reward: 1603.97, Evacuated: 0, Deactivated: 4, Actor Loss: -0.85, Critic Loss: 289.51\n","Episode 176, Reward: 1728.72, Evacuated: 1, Deactivated: 3, Actor Loss: -0.85, Critic Loss: 288.28\n","Episode 177, Reward: 1069.07, Evacuated: 1, Deactivated: 3, Actor Loss: -0.86, Critic Loss: 287.37\n","Episode 178, Reward: 1762.48, Evacuated: 2, Deactivated: 2, Actor Loss: -0.86, Critic Loss: 286.32\n","Episode 179, Reward: 819.83, Evacuated: 1, Deactivated: 3, Actor Loss: -0.86, Critic Loss: 285.53\n","Episode 180, Reward: 2306.72, Evacuated: 3, Deactivated: 1, Actor Loss: -0.87, Critic Loss: 284.46\n","Episode 181, Reward: 2026.72, Evacuated: 2, Deactivated: 2, Actor Loss: -0.87, Critic Loss: 283.40\n","Episode 182, Reward: 1369.09, Evacuated: 2, Deactivated: 2, Actor Loss: -0.88, Critic Loss: 283.02\n","Episode 183, Reward: 469.76, Evacuated: 0, Deactivated: 4, Actor Loss: -0.87, Critic Loss: 283.92\n","Episode 184, Reward: 1168.18, Evacuated: 1, Deactivated: 3, Actor Loss: -0.87, Critic Loss: 282.72\n","Episode 185, Reward: 1936.39, Evacuated: 3, Deactivated: 1, Actor Loss: -0.88, Critic Loss: 281.99\n","Episode 186, Reward: 1617.34, Evacuated: 2, Deactivated: 2, Actor Loss: -0.88, Critic Loss: 281.23\n","Episode 187, Reward: 1461.59, Evacuated: 1, Deactivated: 3, Actor Loss: -0.88, Critic Loss: 280.22\n","Episode 188, Reward: 1020.01, Evacuated: 1, Deactivated: 3, Actor Loss: -0.88, Critic Loss: 279.22\n","Episode 189, Reward: 1354.41, Evacuated: 1, Deactivated: 3, Actor Loss: -0.88, Critic Loss: 278.36\n","Episode 190, Reward: 1231.51, Evacuated: 0, Deactivated: 4, Actor Loss: -0.88, Critic Loss: 277.53\n","Episode 191, Reward: 358.66, Evacuated: 0, Deactivated: 4, Actor Loss: -0.82, Critic Loss: 347.56\n","Episode 192, Reward: 1608.60, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 345.93\n","Episode 193, Reward: 1667.37, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 344.97\n","Episode 194, Reward: 1502.73, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 343.79\n","Episode 195, Reward: 1409.15, Evacuated: 2, Deactivated: 2, Actor Loss: -0.82, Critic Loss: 342.97\n","Episode 196, Reward: 761.04, Evacuated: 0, Deactivated: 4, Actor Loss: -0.82, Critic Loss: 341.74\n","Episode 197, Reward: 1190.04, Evacuated: 0, Deactivated: 4, Actor Loss: -0.81, Critic Loss: 339.57\n","Episode 198, Reward: 1119.42, Evacuated: 0, Deactivated: 4, Actor Loss: -0.81, Critic Loss: 338.25\n","Episode 199, Reward: 390.67, Evacuated: 0, Deactivated: 4, Actor Loss: -0.80, Critic Loss: 337.72\n","Episode 200, Reward: 1487.51, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 336.40\n","Episode 201, Reward: 1467.21, Evacuated: 2, Deactivated: 2, Actor Loss: -0.81, Critic Loss: 335.19\n","Episode 202, Reward: 1369.35, Evacuated: 1, Deactivated: 3, Actor Loss: -0.81, Critic Loss: 334.06\n","Episode 203, Reward: 1599.62, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 333.16\n","Episode 204, Reward: 2095.54, Evacuated: 3, Deactivated: 1, Actor Loss: -0.83, Critic Loss: 332.14\n","Episode 205, Reward: 430.71, Evacuated: 0, Deactivated: 4, Actor Loss: -0.82, Critic Loss: 331.46\n","Episode 206, Reward: 1365.54, Evacuated: 1, Deactivated: 3, Actor Loss: -0.82, Critic Loss: 330.39\n","Episode 207, Reward: 1938.71, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 329.30\n","Episode 208, Reward: 1119.10, Evacuated: 1, Deactivated: 3, Actor Loss: -0.83, Critic Loss: 328.52\n","Episode 209, Reward: 1240.53, Evacuated: 0, Deactivated: 4, Actor Loss: -0.83, Critic Loss: 327.03\n","Episode 210, Reward: 1178.21, Evacuated: 1, Deactivated: 3, Actor Loss: -0.84, Critic Loss: 326.21\n","Episode 211, Reward: 780.07, Evacuated: 0, Deactivated: 4, Actor Loss: -0.83, Critic Loss: 327.33\n","Episode 212, Reward: 1300.59, Evacuated: 0, Deactivated: 4, Actor Loss: -0.82, Critic Loss: 326.20\n","Episode 213, Reward: 1903.33, Evacuated: 3, Deactivated: 1, Actor Loss: -0.82, Critic Loss: 325.23\n","Episode 214, Reward: 1666.38, Evacuated: 2, Deactivated: 2, Actor Loss: -0.83, Critic Loss: 324.21\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-920e090b7772>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Update Actor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-900f32e52304>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, states, actions, log_probs_old, rewards, dones, gamma, clip_eps)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_agent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-900f32e52304>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_agent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/ECM/IAM/Hackathon/Solution/process_state.py\u001b[0m in \u001b[0;36mnormalize_agent_state\u001b[0;34m(self, state_list)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0magent_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0magent_orientation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0magent_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/ECM/IAM/Hackathon/Solution/process_state.py\u001b[0m in \u001b[0;36mnormalize_position\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_agents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_agents\u001b[0m  \u001b[0;31m# Maximum agents within communication range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"\"\"Normalize position (x or y) to [0,1], handle deactivated agents (-1)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["env, agent, config = simulation_config('config.json',new_agent=True)\n","#load agent network\n","agent.actor.load_state_dict(torch.load('actor_best.pth'))\n","agent.critic.load_state_dict(torch.load('critic_best.pth'))\n","n_agents = env.num_agents\n","buffer = ReplayBuffer()\n","\n","for episode in range(1000):\n","    states, info = env.reset()  # (n_agents, obs_dim)\n","    episode_rewards = []\n","\n","    for step in range(500):\n","        actions, log_probs = agent.select_actions(states)  # Récupérer toutes les actions\n","        actions = actions.tolist()\n","        log_probs = log_probs.tolist()\n","        next_states, rewards, dones, _ ,info= env.step(actions)  # Exécuter toutes les actions\n","        #convert to list\n","        # print(dones)\n","        # terminal = list(dones)\n","\n","        # Stocker les expériences pour chaque agent\n","        for i in range(n_agents):\n","            buffer.store((states[i], actions[i], log_probs[i], rewards[i], next_states[i], dones))\n","\n","        states = next_states\n","        episode_rewards.append(sum(rewards))\n","\n","        if dones: break  # Fin de l'épisode si tous les agents sont terminés\n","\n","    # Train the agent after collecting data\n","    data = buffer.get_data()\n","    states, actions, log_probs_old, rewards, next_states, dones = zip(*data)\n","    dones = list(dones)\n","    actor_loss, critic_loss = agent.compute_loss(states, actions, log_probs_old, rewards, dones)\n","\n","    # Update Actor\n","    agent.optim_actor.zero_grad()\n","    actor_loss.backward(retain_graph=True)\n","    agent.optim_actor.step()\n","\n","    # Update Critic\n","    agent.optim_critic.zero_grad()\n","    critic_loss.backward()\n","    agent.optim_critic.step()\n","\n","    print(f\"Episode {episode}, Reward: {sum(episode_rewards):.2f}, Evacuated: {len(info['evacuated_agents'])}, Deactivated: {len(info['deactivated_agents'])}, Actor Loss: {actor_loss:.2f}, Critic Loss: {critic_loss:.2f}\")\n"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"CFHyz71ngIMT","executionInfo":{"status":"ok","timestamp":1741515205390,"user_tz":-60,"elapsed":31,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"}}},"outputs":[],"source":["def evaluate(configs_paths: list, trained_agent, num_episodes: int = 10) -> tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    Evaluate a trained agent on multiple configurations, calculate metrics, and visualize results.\n","\n","    Args:\n","        config_path (list): List of paths to the configuration JSON files.\n","        trained_agent (MyAgent): A pre-trained agent to evaluate.\n","        num_episodes (int): Number of episodes to run for evaluation per configuration. Defaults to 10.\n","\n","    Returns:\n","        pd.DataFrame: A DataFrame containing evaluation metrics for each episode and configuration.\n","    \"\"\"\n","\n","    # Evaluation results\n","    all_results = pd.DataFrame()\n","\n","    for config_path in configs_paths:\n","        print(f\"\\n--- Evaluating Configuration: {config_path} ---\")\n","\n","        # Environment configuration\n","        env, _, config = simulation_config(config_path, new_agent=False)\n","\n","        # Metrics to follow the performance\n","        metrics = []\n","        total_reward = 0\n","        episode_count = 0\n","\n","        # Initial reset of the environment\n","        states, info = env.reset()\n","\n","        evacuated_total = 0\n","        # Run evaluation for the specified number of episodes\n","        try:\n","            while episode_count < num_episodes:\n","                # Determine agents actions\n","                actions, log_probs = trained_agent.select_actions(states)  # Récupérer toutes les actions\n","                actions = actions.tolist()\n","\n","                next_states, rewards, dones, truncated ,info= env.step(actions)  # Exécuter toutes les actions\n","\n","                total_reward += sum(rewards)\n","                # Display of the step information\n","                print(f\"\\rEpisode {episode_count + 1}/{num_episodes}, Step {info['current_step']}, \"\n","                    f\"Reward: {total_reward:.2f}, \"\n","                    f\"Evacuated: {len(info['evacuated_agents'])}, \"\n","                    f\"Deactivated: {len(info['deactivated_agents'])}\", end='')\n","                states = next_states\n","                # Pause\n","                #time.sleep(1)\n","\n","                # If the episode is terminated\n","                if dones or truncated:\n","                    evacuated_total += len(info['evacuated_agents'])\n","                    # Display of the episode information\n","                    print(\"\\r\")\n","                    # Save metrics\n","                    metrics.append({\n","                        \"config_path\": config_path,\n","                        \"episode\": episode_count + 1,\n","                        \"steps\": info['current_step'],\n","                        \"reward\": total_reward,\n","                        \"evacuated\": len(info['evacuated_agents']),\n","                        \"deactivated\": len(info['deactivated_agents'])\n","                    })\n","                    episode_count += 1\n","                    total_reward = 0\n","\n","                    if episode_count < num_episodes:\n","                        states, info = env.reset()\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nSimulation interrupted by the user\")\n","\n","        finally:\n","            env.close()\n","\n","        # Convert the current configuration's metrics to a DataFrame\n","        config_results = pd.DataFrame(metrics)\n","        all_results = pd.concat([all_results, config_results], ignore_index=True)\n","\n","    env.close()\n","\n","    all_results.to_csv('all_results.csv', index=False)\n","\n","    return all_results, evacuated_total"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"f3tXY2ybgIMU","outputId":"896ff813-1770-4add-9bac-e3070ce45a32","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741515276100,"user_tz":-60,"elapsed":48804,"user":{"displayName":"Etienne Delate","userId":"12026614867621159527"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Evaluating Configuration: eval_configs/config_2.json ---\n","Episode 1/10, Step 42, Reward: 914.98, Evacuated: 3, Deactivated: 1\n","Episode 2/10, Step 92, Reward: 502.14, Evacuated: 0, Deactivated: 4\n","Episode 3/10, Step 28, Reward: 363.03, Evacuated: 1, Deactivated: 3\n","Episode 4/10, Step 63, Reward: 257.10, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 99, Reward: 417.16, Evacuated: 0, Deactivated: 4\n","Episode 6/10, Step 22, Reward: 600.00, Evacuated: 2, Deactivated: 2\n","Episode 7/10, Step 28, Reward: 481.05, Evacuated: 1, Deactivated: 3\n","Episode 8/10, Step 91, Reward: 956.09, Evacuated: 2, Deactivated: 2\n","Episode 9/10, Step 126, Reward: 1092.10, Evacuated: 2, Deactivated: 2\n","Episode 10/10, Step 66, Reward: 649.09, Evacuated: 1, Deactivated: 3\n","\n","--- Evaluating Configuration: eval_configs/config_1.json ---\n","Episode 1/10, Step 842, Reward: 3481.06, Evacuated: 4, Deactivated: 0\n","Episode 2/10, Step 944, Reward: 2159.04, Evacuated: 1, Deactivated: 3\n","Episode 3/10, Step 134, Reward: 997.13, Evacuated: 2, Deactivated: 2\n","Episode 4/10, Step 10000, Reward: -5311.05, Evacuated: 2, Deactivated: 0\n","Episode 5/10, Step 27, Reward: 715.01, Evacuated: 2, Deactivated: 2\n","Episode 6/10, Step 123, Reward: 962.19, Evacuated: 2, Deactivated: 2\n","Episode 7/10, Step 67, Reward: 1040.00, Evacuated: 3, Deactivated: 1\n","Episode 8/10, Step 1816, Reward: 1834.36, Evacuated: 2, Deactivated: 2\n","Episode 9/10, Step 294, Reward: 1280.37, Evacuated: 0, Deactivated: 4\n","Episode 10/10, Step 310, Reward: 1550.51, Evacuated: 0, Deactivated: 4\n","\n","--- Evaluating Configuration: eval_configs/config_6.json ---\n","Episode 1/10, Step 34, Reward: 386.07, Evacuated: 0, Deactivated: 4\n","Episode 2/10, Step 38, Reward: 1048.04, Evacuated: 1, Deactivated: 3\n","Episode 3/10, Step 39, Reward: 608.00, Evacuated: 1, Deactivated: 3\n","Episode 4/10, Step 34, Reward: 744.11, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 42, Reward: 918.04, Evacuated: 1, Deactivated: 3\n","Episode 6/10, Step 38, Reward: 1577.92, Evacuated: 3, Deactivated: 1\n","Episode 7/10, Step 36, Reward: 989.04, Evacuated: 1, Deactivated: 3\n","Episode 8/10, Step 37, Reward: 677.01, Evacuated: 1, Deactivated: 3\n","Episode 9/10, Step 21, Reward: 359.06, Evacuated: 0, Deactivated: 4\n","Episode 10/10, Step 42, Reward: 722.11, Evacuated: 0, Deactivated: 4\n","\n","--- Evaluating Configuration: eval_configs/config_3.json ---\n","Episode 1/10, Step 66, Reward: 997.89, Evacuated: 2, Deactivated: 2\n","Episode 2/10, Step 1042, Reward: 5445.00, Evacuated: 2, Deactivated: 2\n","Episode 3/10, Step 38, Reward: 999.01, Evacuated: 2, Deactivated: 2\n","Episode 4/10, Step 39, Reward: 362.08, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 233, Reward: 1813.19, Evacuated: 2, Deactivated: 2\n","Episode 6/10, Step 49, Reward: 641.13, Evacuated: 0, Deactivated: 4\n","Episode 7/10, Step 1065, Reward: 5520.95, Evacuated: 2, Deactivated: 2\n","Episode 8/10, Step 388, Reward: 2410.34, Evacuated: 2, Deactivated: 2\n","Episode 9/10, Step 45, Reward: 867.04, Evacuated: 1, Deactivated: 3\n","Episode 10/10, Step 41, Reward: 1065.01, Evacuated: 2, Deactivated: 2\n","\n","--- Evaluating Configuration: eval_configs/config_10.json ---\n","Episode 1/10, Step 14, Reward: 118.04, Evacuated: 0, Deactivated: 4\n","Episode 2/10, Step 7, Reward: -69.98, Evacuated: 0, Deactivated: 4\n","Episode 3/10, Step 41, Reward: 776.11, Evacuated: 0, Deactivated: 4\n","Episode 4/10, Step 21, Reward: 478.08, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 11, Reward: 18.03, Evacuated: 0, Deactivated: 4\n","Episode 6/10, Step 27, Reward: 569.09, Evacuated: 0, Deactivated: 4\n","Episode 7/10, Step 104, Reward: 877.20, Evacuated: 0, Deactivated: 4\n","Episode 8/10, Step 31, Reward: 758.11, Evacuated: 0, Deactivated: 4\n","Episode 9/10, Step 16, Reward: 198.05, Evacuated: 0, Deactivated: 4\n","Episode 10/10, Step 12, Reward: 58.04, Evacuated: 0, Deactivated: 4\n","\n","--- Evaluating Configuration: eval_configs/config_5.json ---\n","Episode 1/10, Step 41, Reward: 1602.91, Evacuated: 3, Deactivated: 1\n","Episode 2/10, Step 32, Reward: 320.06, Evacuated: 0, Deactivated: 4\n","Episode 3/10, Step 39, Reward: 1804.85, Evacuated: 4, Deactivated: 0\n","Episode 4/10, Step 37, Reward: 1206.98, Evacuated: 2, Deactivated: 2\n","Episode 5/10, Step 39, Reward: 1276.98, Evacuated: 2, Deactivated: 2\n","Episode 6/10, Step 41, Reward: 1167.07, Evacuated: 1, Deactivated: 3\n","Episode 7/10, Step 41, Reward: 1807.85, Evacuated: 4, Deactivated: 0\n","Episode 8/10, Step 37, Reward: 1147.97, Evacuated: 2, Deactivated: 2\n","Episode 9/10, Step 36, Reward: 727.02, Evacuated: 1, Deactivated: 3\n","Episode 10/10, Step 47, Reward: 1390.01, Evacuated: 2, Deactivated: 2\n","\n","--- Evaluating Configuration: eval_configs/config_4.json ---\n","Episode 1/10, Step 32, Reward: 1219.94, Evacuated: 3, Deactivated: 1\n","Episode 2/10, Step 36, Reward: 1080.91, Evacuated: 3, Deactivated: 1\n","Episode 3/10, Step 26, Reward: 510.09, Evacuated: 0, Deactivated: 4\n","Episode 4/10, Step 25, Reward: 397.07, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 26, Reward: 493.08, Evacuated: 0, Deactivated: 4\n","Episode 6/10, Step 34, Reward: 941.99, Evacuated: 2, Deactivated: 2\n","Episode 7/10, Step 25, Reward: 455.01, Evacuated: 1, Deactivated: 3\n","Episode 8/10, Step 26, Reward: 259.99, Evacuated: 1, Deactivated: 3\n","Episode 9/10, Step 26, Reward: 699.03, Evacuated: 1, Deactivated: 3\n","Episode 10/10, Step 26, Reward: 336.00, Evacuated: 1, Deactivated: 3\n","\n","--- Evaluating Configuration: eval_configs/config_8.json ---\n","Episode 1/10, Step 47, Reward: 1010.02, Evacuated: 1, Deactivated: 3\n","Episode 2/10, Step 45, Reward: 918.02, Evacuated: 1, Deactivated: 3\n","Episode 3/10, Step 48, Reward: 1264.16, Evacuated: 0, Deactivated: 4\n","Episode 4/10, Step 33, Reward: 878.12, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 16, Reward: 289.06, Evacuated: 0, Deactivated: 4\n","Episode 6/10, Step 41, Reward: 1197.15, Evacuated: 0, Deactivated: 4\n","Episode 7/10, Step 45, Reward: 998.02, Evacuated: 1, Deactivated: 3\n","Episode 8/10, Step 46, Reward: 729.99, Evacuated: 1, Deactivated: 3\n","Episode 9/10, Step 48, Reward: 1847.89, Evacuated: 3, Deactivated: 1\n","Episode 10/10, Step 80, Reward: 1523.00, Evacuated: 2, Deactivated: 2\n","\n","--- Evaluating Configuration: eval_configs/config_9.json ---\n","Episode 1/10, Step 19, Reward: 268.06, Evacuated: 0, Deactivated: 4\n","Episode 2/10, Step 35, Reward: 700.10, Evacuated: 0, Deactivated: 4\n","Episode 3/10, Step 11, Reward: 29.03, Evacuated: 0, Deactivated: 4\n","Episode 4/10, Step 55, Reward: 697.10, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 27, Reward: 538.08, Evacuated: 0, Deactivated: 4\n","Episode 6/10, Step 24, Reward: 508.08, Evacuated: 0, Deactivated: 4\n","Episode 7/10, Step 26, Reward: 500.08, Evacuated: 0, Deactivated: 4\n","Episode 8/10, Step 32, Reward: 488.08, Evacuated: 0, Deactivated: 4\n","Episode 9/10, Step 39, Reward: 569.09, Evacuated: 0, Deactivated: 4\n","Episode 10/10, Step 19, Reward: 358.07, Evacuated: 0, Deactivated: 4\n","\n","--- Evaluating Configuration: eval_configs/config_7.json ---\n","Episode 1/10, Step 47, Reward: 1817.89, Evacuated: 3, Deactivated: 1\n","Episode 2/10, Step 48, Reward: 1477.96, Evacuated: 2, Deactivated: 2\n","Episode 3/10, Step 49, Reward: 1758.00, Evacuated: 2, Deactivated: 2\n","Episode 4/10, Step 30, Reward: 449.07, Evacuated: 0, Deactivated: 4\n","Episode 5/10, Step 46, Reward: 858.01, Evacuated: 1, Deactivated: 3\n","Episode 6/10, Step 43, Reward: 838.11, Evacuated: 0, Deactivated: 4\n","Episode 7/10, Step 51, Reward: 1504.07, Evacuated: 1, Deactivated: 3\n","Episode 8/10, Step 47, Reward: 1057.03, Evacuated: 1, Deactivated: 3\n","Episode 9/10, Step 47, Reward: 1108.03, Evacuated: 1, Deactivated: 3\n","Episode 10/10, Step 44, Reward: 976.13, Evacuated: 0, Deactivated: 4\n","Total evacuated :  106 / 400\n"]}],"source":["import os\n","# iterate over eval folder\n","evacuated_total = 0\n","for file in os.listdir('eval_configs/'):\n","    _, agents, config = simulation_config(f'eval_configs/{file}', new_agent=True)\n","    #turn off warning\n","    import warnings\n","    warnings.filterwarnings(\"ignore\")\n","    agents.actor.load_state_dict(torch.load('actor_best.pth'))\n","    agents.critic.load_state_dict(torch.load('critic_best.pth'))\n","    eval_configs = [f'eval_configs/{file}']\n","    all_results, evacuated = evaluate(configs_paths=eval_configs, trained_agent=agent)\n","    evacuated_total += evacuated\n","print(\"Total evacuated : \",evacuated_total,\"/\",400)"]},{"cell_type":"code","source":["#save networks\n","torch.save(agent.actor.state_dict(), 'actor_best.pth')\n","torch.save(agent.critic.state_dict(), 'critic_best.pth')"],"metadata":{"id":"iBeVjMWWlECp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgQzkRyfgIMU"},"outputs":[],"source":["class PPO():\n","\n","    def __init__(\n","            self,\n","            gamma: float = 0.99,\n","            lam: float = 0.95,\n","            lr_actor: float = 1e-3,\n","            lr_critic: float = 1e-3,\n","            max_episode_len: float = 1000,\n","            batch_size: int = 512,\n","            steps_per_epoch: int = 2048,\n","            clip_ratio: float = 0.2,\n","    ) -> None:\n","\n","        super().__init__()\n","\n","        # Hyperparameters\n","        self.lr_actor = lr_actor\n","        self.lr_critic = lr_critic\n","        self.steps_per_epoch = steps_per_epoch\n","        self.batch_size = batch_size\n","        self.gamma = gamma  # only needed for trajectory\n","        self.lam = lam  # only needed for trajectory\n","        self.max_episode_len = max_episode_len\n","        self.clip_ratio = clip_ratio  # epsilon\n","        self.automatic_optimization = False\n","        self.save_hyperparameters()\n","\n","        #self.env = gym.make(env, render_mode='rgb_array')\n","        self.critic = create_mlp(self.env.observation_space.shape, 1)\n","        actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\n","        self.actor = DiscreteActor(actor_mlp)\n","\n","        self.agent = ActorCritic(self.actor, self.critic)\n","\n","        self.batch_states = []\n","        self.batch_actions = []\n","        self.batch_adv = []\n","        self.batch_d_rewards = []\n","        self.batch_logp = []\n","\n","        self.ep_rewards = []\n","        self.ep_values = []\n","        self.epoch_rewards = []\n","\n","        self.episode_step = 0\n","        self.avg_ep_reward = 0\n","        self.avg_ep_len = 0\n","        self.avg_reward = 0\n","\n","        self.state = torch.FloatTensor(self.env.reset()[0])\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Passes in a state x through the network and returns the policy and a sampled action\n","        Args:\n","            x: environment state\n","        Returns:\n","            Tuple of policy and action\n","        \"\"\"\n","        action_prob, action = self.actor(x)\n","        value = self.critic(x)\n","\n","        return action_prob, action, value\n","\n","    def actor_loss(self, state, action, logp_old, advantage) -> torch.Tensor:\n","        \"\"\"\n","        Calculate the actor loss.\n","\n","        Args:\n","            state: current state of environment\n","            action: selected action\n","            logp_old: old log-probability\n","            advantage: advantage of action\n","        Returns:\n","            loss\n","        \"\"\"\n","\n","        # TODO: Implement the PPO Actor Loss\n","        pi,_ = self.actor.forward(state)\n","        logpi = pi.log_prob(action)\n","\n","        quotient = torch.exp(logpi)/torch.exp(logp_old)\n","        new_adv = torch.clamp(quotient,1-self.clip_ratio,1+self.clip_ratio)*advantage\n","        f = lambda x: x if x < 1+self.clip_ratio else 1+self.clip_ratio\n","        with torch.no_grad():\n","            quotient.data.apply_(f)\n","\n","        loss_actor = -(quotient*advantage).mean()\n","\n","\n","        return loss_actor\n","\n","    def critic_loss(self, state: torch.Tensor, d_reward: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Calculate the critic loss.\n","\n","        Args:\n","            state: current state of environment\n","            d_reward: discounted reward\n","        Returns:\n","            loss\n","        \"\"\"\n","        # TODO: Implemente the PPO Critic Loss\n","        value = self.agent(state=state)[-1]\n","        loss_critic = ((value - d_reward)**2).mean()\n","\n","        return loss_critic\n","\n","    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx):\n","        \"\"\"\n","        Carries out a single update to actor and critic network from a batch of replay buffer.\n","\n","        Args:\n","            batch: batch of replay buffer/trajectory data\n","            batch_idx: used for logging\n","            optimizer_idx: idx that controls optimizing actor or critic network\n","        Returns:\n","            loss\n","        \"\"\"\n","        optims = self.optimizers()\n","        optim = optims[0] if batch_idx % 2 == 0 else optims[1]\n","        optim.zero_grad()\n","\n","        state, action, old_logp, d_reward, advantage = batch\n","\n","        # normalize advantages\n","        advantage = (advantage - advantage.mean()) / advantage.std()\n","\n","        self.log(\"avg_ep_len\", self.trainer.datamodule.avg_ep_len, prog_bar=True, on_step=False, on_epoch=True)\n","        self.log(\"avg_ep_reward\", self.trainer.datamodule.avg_ep_reward, prog_bar=True, on_step=False, on_epoch=True)\n","        self.log(\"avg_reward\", self.trainer.datamodule.avg_reward, prog_bar=True, on_step=False, on_epoch=True)\n","\n","        if batch_idx % 2 == 0:\n","            loss_actor = self.actor_loss(state, action, old_logp, advantage)\n","            self.log('loss_actor', loss_actor, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","            self.manual_backward(loss_actor)\n","            optim.step()\n","\n","            return loss_actor\n","\n","        elif batch_idx % 2 == 0:\n","            loss_critic = self.critic_loss(state, d_reward)\n","            self.log('loss_critic', loss_critic, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n","            self.manual_backward(loss_critic)\n","            optim.step()\n","\n","            return loss_critic\n","\n","    def viz_agent(self):\n","        '''\n","        Visualize actions of the trained agent in environment in a loop, you should use the built in env.render() functionality\n","        :return:\n","        '''\n","\n","        imgs = []\n","        fig = plt.figure()\n","        state = torch.FloatTensor(self.env.reset()[0])\n","        img = self.env.render()\n","        imgs.append(img)\n","\n","         # TODO: implement full evaluation loop of environment and use env.render() to get images\n","        state_imgs = []\n","        for _ in range(self.max_episode_len):\n","            _, action, _, _ = self.agent(state)\n","            new_state,_,terminated,*_ = self.env.step(action.cpu().numpy())\n","            state = torch.FloatTensor(new_state)\n","            if terminated:\n","                break\n","            state_imgs.append(self.env.render())\n","        imgs += state_imgs\n","\n","        im = plt.imshow(imgs[0])\n","        print('Episode length', len(imgs))\n","        def animate(i):\n","            im.set_array((imgs[i]))\n","            return [im]\n","\n","        anim = FuncAnimation(fig, animate, frames=len(imgs), interval=20)\n","        return anim\n","\n","\n","    def configure_optimizers(self) -> List[Optimizer]:\n","        \"\"\" Initialize Adam optimizer\"\"\"\n","        optimizer_actor = optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n","        optimizer_critic = optim.Adam(self.critic.parameters(), lr=self.lr_critic)\n","\n","        return optimizer_actor, optimizer_critic"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
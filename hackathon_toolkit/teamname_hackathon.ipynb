{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d271c0b8",
   "metadata": {},
   "source": [
    "# Hackathon : Reinforcement Learning for Drone Navigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b59b4fd",
   "metadata": {},
   "source": [
    "- Team name:\n",
    "- Team members names:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb5f6162-b5c3-462f-8c79-1fe99d16f175",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68618ae7-0858-4006-be28-cd1451979861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dbd81fc-3ed3-4ecc-92da-ea4a86bb6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import env\n",
    "import agent\n",
    "import reward\n",
    "import simulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca2c8db-cf8d-46d0-b8bb-ef4dba40009f",
   "metadata": {},
   "source": [
    "## 1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa34aa4c-0eab-4db9-9b6b-3fe1f714190c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could only place 0 dynamic obstacles instead of 3 due to space constraints\n",
      "[1. 3.]\n",
      "REWARD  2 [array([4, 4]), array([3, 4])] [array([4, 4]), array([3, 4])] set() set() [(0, 0), (1, 0)]\n",
      "N_step total =  1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_agent, all_rewards, all_results \u001b[39m=\u001b[39m simulate\u001b[39m.\u001b[39;49mtrain(\u001b[39m'\u001b[39;49m\u001b[39mconfig.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/cours/AIRBUS_hackathon/hackathon_toolkit/simulate.py:111\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m n_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[39m# print(\"STATE1 : \", state)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# print(\"ACTION :\", actions)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# Execution of a simulation step\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m state, rewards, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    112\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(rewards)\n\u001b[1;32m    113\u001b[0m \u001b[39m# print(\"STATE2 : \", state)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[39m# Update agent policy\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/cours/AIRBUS_hackathon/hackathon_toolkit/env.py:421\u001b[0m, in \u001b[0;36mMazeEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeactivated_agents \u001b[39m|\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevacuated_agents) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_agents:\n\u001b[1;32m    419\u001b[0m     episode_ending \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_reward(old_positions)\n\u001b[1;32m    423\u001b[0m \u001b[39m# Prepare state and informations\u001b[39;00m\n\u001b[1;32m    424\u001b[0m state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_agent_state(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_agents)])\n",
      "File \u001b[0;32m~/Documents/cours/AIRBUS_hackathon/hackathon_toolkit/env.py:340\u001b[0m, in \u001b[0;36mMazeEnv.get_reward\u001b[0;34m(self, old_positions)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_reward\u001b[39m(\u001b[39mself\u001b[39m, old_positions: \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 340\u001b[0m     rewards, evacuated_agents \u001b[39m=\u001b[39m compute_reward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_agents, old_positions,\n\u001b[1;32m    341\u001b[0m                                                \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_positions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevacuated_agents, \n\u001b[1;32m    342\u001b[0m                                                \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeactivated_agents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal_area)\n\u001b[1;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m evacuated_agents \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevacuated_agents:\n\u001b[1;32m    344\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevacuated_agents \u001b[39m=\u001b[39m evacuated_agents    \n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "trained_agent, all_rewards, all_results = simulate.train('config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the cumulated rewards per episode\n",
    "simulate.plot_cumulated_rewards(all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cumulative_counts(data):\n",
    "\n",
    "    x = np.arange(len(data))  # Indices de la liste\n",
    "    y1, y2 = [], []\n",
    "\n",
    "    # Calculer les sommes cumulées pour chaque composante des tuples\n",
    "    cumulative_sum_1 = 0\n",
    "    cumulative_sum_2 = 0\n",
    "    for t in data:\n",
    "        cumulative_sum_1 += t[0]\n",
    "        cumulative_sum_2 += t[1]\n",
    "        y1.append(cumulative_sum_1)\n",
    "        y2.append(cumulative_sum_2)\n",
    "\n",
    "    # Tracer les courbes cumulées\n",
    "    plt.plot(x, y1, label=\"Cumsum evacuated agent\", color='green')\n",
    "    plt.plot(x, y2, label=\"cumsum deactivated agent\", color='red')\n",
    "\n",
    "    # Ajouter des labels et une légende\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cumulative Value\")\n",
    "    plt.legend()\n",
    "\n",
    "# Afficher le graphiq\n",
    "\n",
    "plot_cumulative_counts(all_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2ac3652-834a-4943-a2f6-96dc395a21ae",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16713487-5971-4789-bc72-08bc1fb2dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_config_paths = [f\"./eval_configs/config_{i}.json\" for i in range(1, 11)]\n",
    "eval_config_paths = [f\"./config.json\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ddc0a-19ce-4e67-996f-a19432201b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_results = simulate.evaluate(eval_config_paths, trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82856a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averages for each configuration\n",
    "averages = all_results.groupby('config_path').mean().reset_index().drop(columns=['episode'])\n",
    "averages = averages.rename(columns={\n",
    "    'steps': 'avg_steps',\n",
    "    'reward': 'avg_reward',\n",
    "    'evacuated': 'avg_evacuated',\n",
    "    'deactivated': 'avg_deactivated'})\n",
    "\n",
    "display(averages)\n",
    "averages.to_csv('averages.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "9240d949b7e875368571ba59acc67192d2efbcc4561b3c6f94c83d7858e18732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import env\n",
    "import agent\n",
    "import reward\n",
    "import simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 229\u001b[39m\n\u001b[32m    223\u001b[39m     all_results.to_csv(\u001b[33m'\u001b[39m\u001b[33mall_results.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(config_path)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m episode_count < max_episodes:\n\u001b[32m     98\u001b[39m         \u001b[38;5;66;03m# Determine agents actions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m         actions = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m         \u001b[38;5;28mprint\u001b[39m(actions)\n\u001b[32m    101\u001b[39m         \u001b[38;5;66;03m# Execution of a simulation step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:6\u001b[39m, in \u001b[36mget_action\u001b[39m\u001b[34m(self, list_state, evaluation)\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "from env import MazeEnv\n",
    "from agent import MyAgent\n",
    "\n",
    "\n",
    "def simulation_config(config_path: str, new_agent: bool = True) -> Tuple[MazeEnv, Optional[MyAgent], Dict]:\n",
    "    \"\"\"\n",
    "    Configure the environment and optionally an agent using a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the configuration JSON file.\n",
    "        new_agent (bool): Whether to initialize the agent. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[MazeEnv, Optional[MyAgent], Dict]: Configured environment, agent (if new), and the configuration dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read config\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Env configuration\n",
    "    env = MazeEnv(\n",
    "        size=config.get('grid_size'),                               # Grid size\n",
    "        walls_proportion=config.get('walls_proportion'),            # Walls proportion in the grid\n",
    "        num_dynamic_obstacles=config.get('num_dynamic_obstacles'),  # Number of dynamic obstacles\n",
    "        num_agents=config.get('num_agents'),                        # Number of agents\n",
    "        communication_range=config.get('communication_range'),      # Maximum distance for agent communications\n",
    "        max_lidar_dist_main=config.get('max_lidar_dist_main'),      # Maximum distance for main LIDAR scan\n",
    "        max_lidar_dist_second=config.get('max_lidar_dist_second'),  # Maximum distance for secondary LIDAR scan\n",
    "        max_episode_steps=config.get('max_episode_steps'),          # Number of steps before episode termination\n",
    "        render_mode=config.get('render_mode', None),\n",
    "        seed=config.get('seed', None)                               # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Agent configuration\n",
    "    agent = MyAgent(num_agents=config.get('num_agents')) if new_agent else None\n",
    "\n",
    "    return env, agent, config\n",
    "\n",
    "\n",
    "def plot_cumulated_rewards(rewards: list, interval: int = 100):\n",
    "    \"\"\"\n",
    "    Plot and save the rewards over episodes.\n",
    "\n",
    "    Args:\n",
    "        rewards (list): List of total rewards per episode.\n",
    "        interval (int): Interval between ticks on the x-axis (default is 100).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(rewards)+1), rewards, color='blue', marker='o', linestyle='-')\n",
    "    plt.title('Total Cumulated Rewards per Episode')\n",
    "    plt.xlabel('Episodes')\n",
    "    \n",
    "    # Adjust x-ticks to display every 'interval' episodes\n",
    "    xticks = range(1, len(rewards)+1, interval)\n",
    "    plt.xticks(xticks)\n",
    "    \n",
    "    plt.ylabel('Cumulated Rewards')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('reward_curve_per_episode.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train(config_path: str) -> MyAgent:\n",
    "    \"\"\"\n",
    "    Train an agent on the configured environment.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the configuration JSON file.\n",
    "\n",
    "    Returns:\n",
    "        MyAgent: The trained agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Environment and agent configuration\n",
    "    env, agent, config = simulation_config(config_path)\n",
    "    max_episodes = config.get('max_episodes')\n",
    "\n",
    "    # Metrics to follow the performance\n",
    "    all_rewards = []\n",
    "    total_reward = 0\n",
    "    episode_count = 0\n",
    "    \n",
    "    # Initial reset of the environment\n",
    "    state, info = env.reset()\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        while episode_count < max_episodes:\n",
    "            # Determine agents actions\n",
    "            actions = agent.get_action(state)\n",
    "            print(actions)\n",
    "            # Execution of a simulation step\n",
    "            state, rewards, terminated, truncated, info = env.step(actions)\n",
    "            print(state)\n",
    "            return state\n",
    "            print(rewards)\n",
    "            # print( terminated)\n",
    "            # print( truncated)\n",
    "            # print(info)\n",
    "            total_reward += np.sum(rewards)\n",
    "\n",
    "            # Update agent policy\n",
    "            agent.update_policy(actions, state, rewards)\n",
    "\n",
    "            # Display of the step information\n",
    "            print(f\"\\rEpisode {episode_count + 1}, Step {info['current_step']}, \"\n",
    "                  f\"Reward: {total_reward:.2f}, \"\n",
    "                  f\"Evacuated: {len(info['evacuated_agents'])}, \"\n",
    "                  f\"Deactivated: {len(info['deactivated_agents'])}\", end='')\n",
    "            \n",
    "            # Pause\n",
    "            # time.sleep(1)\n",
    "            break\n",
    "            \n",
    "            # If the episode is terminated\n",
    "            if terminated or truncated:\n",
    "                print(\"\\r\")\n",
    "                episode_count += 1\n",
    "                all_rewards.append(total_reward)\n",
    "                total_reward = 0\n",
    "                \n",
    "                if episode_count < max_episodes:\n",
    "                    state, info = env.reset()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSimulation interrupted by the user\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "    return agent, all_rewards\n",
    "\n",
    "\n",
    "def evaluate(configs_paths: list, trained_agent: MyAgent, num_episodes: int = 10) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained agent on multiple configurations, calculate metrics, and visualize results.\n",
    "\n",
    "    Args:\n",
    "        config_path (list): List of paths to the configuration JSON files.\n",
    "        trained_agent (MyAgent): A pre-trained agent to evaluate.\n",
    "        num_episodes (int): Number of episodes to run for evaluation per configuration. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing evaluation metrics for each episode and configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluation results\n",
    "    all_results = pd.DataFrame()\n",
    "\n",
    "    for config_path in configs_paths:\n",
    "        print(f\"\\n--- Evaluating Configuration: {config_path} ---\")\n",
    "\n",
    "        # Environment configuration\n",
    "        env, _, config = simulation_config(config_path, new_agent=False)\n",
    "\n",
    "        # Metrics to follow the performance\n",
    "        metrics = []\n",
    "        total_reward = 0\n",
    "        episode_count = 0\n",
    "        \n",
    "        # Initial reset of the environment\n",
    "        state, info = env.reset()\n",
    "        time.sleep(1) \n",
    "   \n",
    "        # Run evaluation for the specified number of episodes\n",
    "        try:\n",
    "            while episode_count < num_episodes:\n",
    "                # Determine agents actions\n",
    "                actions = trained_agent.get_action(state, evaluation=True)\n",
    "\n",
    "                # Execution of a simulation step\n",
    "                state, rewards, terminated, truncated, info = env.step(actions)\n",
    "                total_reward += np.sum(rewards)\n",
    "\n",
    "                # Display of the step information\n",
    "                print(f\"\\rEpisode {episode_count + 1}/{num_episodes}, Step {info['current_step']}, \"\n",
    "                    f\"Reward: {total_reward:.2f}, \"\n",
    "                    f\"Evacuated: {len(info['evacuated_agents'])}, \"\n",
    "                    f\"Deactivated: {len(info['deactivated_agents'])}\", end='')\n",
    "            \n",
    "                # Pause\n",
    "                time.sleep(1)\n",
    "\n",
    "                # If the episode is terminated\n",
    "                if terminated or truncated:\n",
    "                    print(\"\\r\")\n",
    "                    # Save metrics\n",
    "                    metrics.append({\n",
    "                        \"config_path\": config_path,\n",
    "                        \"episode\": episode_count + 1,\n",
    "                        \"steps\": info['current_step'],\n",
    "                        \"reward\": total_reward,\n",
    "                        \"evacuated\": len(info['evacuated_agents']),\n",
    "                        \"deactivated\": len(info['deactivated_agents'])\n",
    "                    })\n",
    "\n",
    "                    episode_count += 1\n",
    "                    total_reward = 0\n",
    "\n",
    "                    if episode_count < num_episodes:\n",
    "                        state, info = env.reset()\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nSimulation interrupted by the user\")\n",
    "        \n",
    "        finally:\n",
    "            env.close()\n",
    "\n",
    "        # Convert the current configuration's metrics to a DataFrame\n",
    "        config_results = pd.DataFrame(metrics)\n",
    "        all_results = pd.concat([all_results, config_results], ignore_index=True)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    all_results.to_csv('all_results.csv', index=False)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "\n",
    "state = train(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstate\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import env\n",
    "import agent\n",
    "import reward\n",
    "import simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could only place 0 dynamic obstacles instead of 3 due to space constraints\n",
      "PREPRO :  [array([4, 4, 0, 2, 1, 1, 1, 2, 1, 4, 4]), array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "action 0  [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[0, 0]\n",
      "[[4. 4. 0. 0. 0. 0. 2. 1. 1. 1. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "from env import MazeEnv\n",
    "from agent import MyAgent\n",
    "\n",
    "\n",
    "def simulation_config(config_path: str, new_agent: bool = True) -> Tuple[MazeEnv, Optional[MyAgent], Dict]:\n",
    "    \"\"\"\n",
    "    Configure the environment and optionally an agent using a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the configuration JSON file.\n",
    "        new_agent (bool): Whether to initialize the agent. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[MazeEnv, Optional[MyAgent], Dict]: Configured environment, agent (if new), and the configuration dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read config\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Env configuration\n",
    "    env = MazeEnv(\n",
    "        size=config.get('grid_size'),                               # Grid size\n",
    "        walls_proportion=config.get('walls_proportion'),            # Walls proportion in the grid\n",
    "        num_dynamic_obstacles=config.get('num_dynamic_obstacles'),  # Number of dynamic obstacles\n",
    "        num_agents=config.get('num_agents'),                        # Number of agents\n",
    "        communication_range=config.get('communication_range'),      # Maximum distance for agent communications\n",
    "        max_lidar_dist_main=config.get('max_lidar_dist_main'),      # Maximum distance for main LIDAR scan\n",
    "        max_lidar_dist_second=config.get('max_lidar_dist_second'),  # Maximum distance for secondary LIDAR scan\n",
    "        max_episode_steps=config.get('max_episode_steps'),          # Number of steps before episode termination\n",
    "        render_mode=config.get('render_mode', None),\n",
    "        seed=config.get('seed', None)                               # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Agent configuration\n",
    "    agent = MyAgent(num_agents=config.get('num_agents')) if new_agent else None\n",
    "\n",
    "    return env, agent, config\n",
    "\n",
    "\n",
    "def plot_cumulated_rewards(rewards: list, interval: int = 100):\n",
    "    \"\"\"\n",
    "    Plot and save the rewards over episodes.\n",
    "\n",
    "    Args:\n",
    "        rewards (list): List of total rewards per episode.\n",
    "        interval (int): Interval between ticks on the x-axis (default is 100).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(rewards)+1), rewards, color='blue', marker='o', linestyle='-')\n",
    "    plt.title('Total Cumulated Rewards per Episode')\n",
    "    plt.xlabel('Episodes')\n",
    "    \n",
    "    # Adjust x-ticks to display every 'interval' episodes\n",
    "    xticks = range(1, len(rewards)+1, interval)\n",
    "    plt.xticks(xticks)\n",
    "    \n",
    "    plt.ylabel('Cumulated Rewards')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('reward_curve_per_episode.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train(config_path: str) -> MyAgent:\n",
    "    \"\"\"\n",
    "    Train an agent on the configured environment.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the configuration JSON file.\n",
    "\n",
    "    Returns:\n",
    "        MyAgent: The trained agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Environment and agent configuration\n",
    "    env, agent, config = simulation_config(config_path)\n",
    "    max_episodes = config.get('max_episodes')\n",
    "\n",
    "    # Metrics to follow the performance\n",
    "    all_rewards = []\n",
    "    total_reward = 0\n",
    "    episode_count = 0\n",
    "    \n",
    "    # Initial reset of the environment\n",
    "    state, info = env.reset()\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        while episode_count < max_episodes:\n",
    "            # Determine agents actions\n",
    "            print(\"STATE : \", state)\n",
    "            prepro = agent.process_states(state)\n",
    "            print(\"PREPRO : \", prepro)\n",
    "            actions = agent.get_action(state)\n",
    "            print(actions)\n",
    "            # Execution of a simulation step\n",
    "            state, rewards, terminated, truncated, info = env.step(actions)\n",
    "            print(state)\n",
    "            return state\n",
    "            print(rewards)\n",
    "            # print( terminated)\n",
    "            # print( truncated)\n",
    "            # print(info)\n",
    "            total_reward += np.sum(rewards)\n",
    "\n",
    "            # Update agent policy\n",
    "            agent.update_policy(actions, state, rewards)\n",
    "\n",
    "            # Display of the step information\n",
    "            print(f\"\\rEpisode {episode_count + 1}, Step {info['current_step']}, \"\n",
    "                  f\"Reward: {total_reward:.2f}, \"\n",
    "                  f\"Evacuated: {len(info['evacuated_agents'])}, \"\n",
    "                  f\"Deactivated: {len(info['deactivated_agents'])}\", end='')\n",
    "    except: pass\n",
    "\n",
    "\n",
    "state = train(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29. 29.  0.  0.  0.  0.  1.  3.  1.  1.  1.  3. 28. 29.  0.  0.  4.  1.\n",
      "   1.  1.  1.  3. 29. 28.  1.  0.  1.  3.  1.  1.  1.  3. 28. 28.  1.  0.\n",
      "   1.  3.  1.  3.  1.  1.]\n",
      " [28. 29.  0.  0.  1.  0.  4.  1.  1.  1.  1.  3. 29. 29.  0.  0.  1.  3.\n",
      "   1.  1.  1.  3. 29. 28.  1.  0.  1.  3.  1.  1.  1.  3. 28. 28.  1.  0.\n",
      "   1.  3.  1.  3.  1.  1.]\n",
      " [29. 28.  1.  0.  0.  1.  1.  3.  1.  1.  1.  3. 29. 29.  0.  0.  1.  3.\n",
      "   1.  1.  1.  3. 28. 29.  0.  0.  4.  1.  1.  1.  1.  3. 28. 28.  1.  0.\n",
      "   1.  3.  1.  3.  1.  1.]\n",
      " [28. 28.  1.  0.  1.  1.  1.  3.  1.  3.  1.  1. 29. 29.  0.  0.  1.  3.\n",
      "   1.  1.  1.  3. 28. 29.  0.  0.  4.  1.  1.  1.  1.  3. 29. 28.  1.  0.\n",
      "   1.  3.  1.  1.  1.  3.]]\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

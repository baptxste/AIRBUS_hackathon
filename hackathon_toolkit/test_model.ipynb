{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from typing import Tuple, Optional, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from env import MazeEnv\n",
    "from agent import MyAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_config(config_path: str, new_agent: bool = True) -> Tuple[MazeEnv, Optional[MyAgents], Dict]:\n",
    "    \"\"\"\n",
    "    Configure the environment and optionally an agent using a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the configuration JSON file.\n",
    "        new_agent (bool): Whether to initialize the agent. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[MazeEnv, Optional[MyAgent], Dict]: Configured environment, agent (if new), and the configuration dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read config\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Env configuration\n",
    "    env = MazeEnv(\n",
    "        size=config.get('grid_size'),                               # Grid size\n",
    "        walls_proportion=config.get('walls_proportion'),            # Walls proportion in the grid\n",
    "        num_dynamic_obstacles=config.get('num_dynamic_obstacles'),  # Number of dynamic obstacles\n",
    "        num_agents=config.get('num_agents'),                        # Number of agents\n",
    "        communication_range=config.get('communication_range'),      # Maximum distance for agent communications\n",
    "        max_lidar_dist_main=config.get('max_lidar_dist_main'),      # Maximum distance for main LIDAR scan\n",
    "        max_lidar_dist_second=config.get('max_lidar_dist_second'),  # Maximum distance for secondary LIDAR scan\n",
    "        max_episode_steps=config.get('max_episode_steps'),          # Number of steps before episode termination\n",
    "        render_mode=config.get('render_mode', None),\n",
    "        seed=config.get('seed', None)                               # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Agent configuration\n",
    "    agent = MyAgents(num_agents=config.get('num_agents'), state_dim=42,action_dim=7) if new_agent else None\n",
    "\n",
    "    return env, agent, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Step 86, Reward: -11423.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 2, Step 73, Reward: -16812.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 3, Step 69, Reward: -8619.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 4, Step 35, Reward: -6107.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 5, Step 9, Reward: -2001.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 6, Step 61, Reward: -16807.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 7, Step 29, Reward: -4407.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 8, Step 45, Reward: -9308.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 9, Step 43, Reward: -7809.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 10, Step 42, Reward: -8608.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 11, Step 54, Reward: -12609.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 12, Step 32, Reward: -6206.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 13, Step 51, Reward: -6414.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 14, Step 33, Reward: -7705.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 15, Step 54, Reward: -5616.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 16, Step 26, Reward: -4905.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 17, Step 48, Reward: -5813.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 18, Step 25, Reward: -4405.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 19, Step 33, Reward: -5208.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 20, Step 51, Reward: -8112.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 21, Step 26, Reward: -4206.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 22, Step 65, Reward: -11015.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 23, Step 43, Reward: -6410.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 24, Step 28, Reward: -5405.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 25, Step 17, Reward: -1805.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 26, Step 28, Reward: -6404.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 27, Step 34, Reward: -8804.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 28, Step 26, Reward: -5804.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 29, Step 17, Reward: -2903.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 30, Step 26, Reward: -6204.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 31, Step 63, Reward: -12213.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 32, Step 56, Reward: -10511.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 33, Step 102, Reward: -14626.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 34, Step 47, Reward: -9409.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 35, Step 26, Reward: -4006.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 36, Step 41, Reward: -6809.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 37, Step 32, Reward: -6406.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 38, Step 76, Reward: -11019.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 39, Step 80, Reward: -20811.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 40, Step 42, Reward: -8208.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 41, Step 24, Reward: -5004.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 42, Step 71, Reward: -15612.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 43, Step 17, Reward: -1805.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 44, Step 31, Reward: -6705.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 45, Step 62, Reward: -13411.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 46, Step 22, Reward: -4104.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 47, Step 29, Reward: -6804.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 48, Step 35, Reward: -5408.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 49, Step 40, Reward: -7508.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 50, Step 18, Reward: -4502.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 51, Step 96, Reward: -15323.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 52, Step 50, Reward: -6413.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 53, Step 20, Reward: -2006.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 54, Step 36, Reward: -3311.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 55, Step 29, Reward: -3408.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 56, Step 83, Reward: -20912.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 57, Step 47, Reward: -13305.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 58, Step 22, Reward: -3505.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 59, Step 44, Reward: -6710.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 60, Step 42, Reward: -4911.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 61, Step 106, Reward: -15227.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 62, Step 33, Reward: -6207.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 63, Step 15, Reward: -2903.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 64, Step 104, Reward: -11230.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 65, Step 38, Reward: -4810.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 66, Step 24, Reward: -4405.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 67, Step 76, Reward: -16613.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 68, Step 154, Reward: -30331.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 69, Step 41, Reward: -6609.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 70, Step 25, Reward: -4205.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 71, Step 71, Reward: -18609.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 72, Step 27, Reward: -5105.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 73, Step 48, Reward: -9409.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 74, Step 63, Reward: -10714.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 75, Step 29, Reward: -3008.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 76, Step 78, Reward: -12119.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 77, Step 84, Reward: -12021.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 78, Step 31, Reward: -3508.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 79, Step 60, Reward: -10713.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 80, Step 60, Reward: -10913.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 81, Step 13, Reward: -2802.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 82, Step 21, Reward: -5602.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 83, Step 22, Reward: -4504.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 84, Step 22, Reward: -5103.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 85, Step 79, Reward: -20111.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 86, Step 29, Reward: -5106.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 87, Step 62, Reward: -12012.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 88, Step 39, Reward: -6609.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 89, Step 54, Reward: -11909.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 90, Step 27, Reward: -2608.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 91, Step 70, Reward: -15812.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 92, Step 146, Reward: -36721.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 93, Step 118, Reward: -27619.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 94, Step 30, Reward: -6605.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 95, Step 49, Reward: -6513.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 96, Step 28, Reward: -6105.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 97, Step 49, Reward: -7612.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 98, Step 30, Reward: -7604.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 99, Step 62, Reward: -12812.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 100, Step 47, Reward: -11207.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 101, Step 49, Reward: -12107.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 102, Step 30, Reward: -7604.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 103, Step 32, Reward: -8004.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 104, Step 19, Reward: -2904.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 105, Step 18, Reward: -3903.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 106, Step 31, Reward: -6106.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 107, Step 32, Reward: -5307.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 108, Step 29, Reward: -5406.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 109, Step 80, Reward: -21210.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 110, Step 86, Reward: -21512.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 111, Step 26, Reward: -6503.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 112, Step 32, Reward: -4708.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 113, Step 48, Reward: -11407.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 114, Step 27, Reward: -3906.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 115, Step 31, Reward: -5207.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 116, Step 32, Reward: -6306.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 117, Step 29, Reward: -6105.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 118, Step 41, Reward: -7409.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 119, Step 26, Reward: -6004.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 120, Step 55, Reward: -15206.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 121, Step 40, Reward: -10605.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 122, Step 102, Reward: -25115.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 123, Step 40, Reward: -7608.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 124, Step 54, Reward: -13008.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 125, Step 125, Reward: -31118.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 126, Step 29, Reward: -3807.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 127, Step 12, Reward: -2302.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 128, Step 63, Reward: -14211.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 129, Step 73, Reward: -16113.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 130, Step 37, Reward: -2412.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 131, Step 32, Reward: -6905.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 132, Step 16, Reward: -2903.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 133, Step 45, Reward: -6012.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 134, Step 60, Reward: -12711.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 135, Step 117, Reward: -24822.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 136, Step 50, Reward: -7512.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 137, Step 110, Reward: -25718.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 138, Step 54, Reward: -13008.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 139, Step 20, Reward: -3204.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 140, Step 48, Reward: -8810.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 141, Step 49, Reward: -10509.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 142, Step 37, Reward: -7807.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 143, Step 26, Reward: -4805.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 144, Step 85, Reward: -21712.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 145, Step 25, Reward: -4105.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 146, Step 24, Reward: -5104.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 147, Step 74, Reward: -13715.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 148, Step 47, Reward: -6911.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 149, Step 26, Reward: -4006.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 150, Step 24, Reward: -6303.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 151, Step 13, Reward: -2802.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 152, Step 35, Reward: -9504.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 153, Step 28, Reward: -6404.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 154, Step 39, Reward: -10505.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 155, Step 39, Reward: -9106.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 156, Step 28, Reward: -6304.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 157, Step 11, Reward: -2202.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 158, Step 17, Reward: -1904.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 159, Step 24, Reward: -5304.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 160, Step 32, Reward: -8004.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 161, Step 36, Reward: -5708.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 162, Step 15, Reward: -3602.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 163, Step 18, Reward: -2404.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 164, Step 19, Reward: -3504.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 165, Step 46, Reward: -9209.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 166, Step 34, Reward: -2511.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 167, Step 94, Reward: -20716.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 168, Step 37, Reward: -5609.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 169, Step 74, Reward: -12617.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 170, Step 37, Reward: -8706.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 171, Step 34, Reward: -5108.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 172, Step 33, Reward: -5108.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 173, Step 38, Reward: -8606.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 174, Step 58, Reward: -12510.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 175, Step 8, Reward: -1801.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 176, Step 38, Reward: -10205.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 177, Step 75, Reward: -16413.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 178, Step 19, Reward: -3703.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 179, Step 35, Reward: -6107.90, Evacuated: 0, Deactivated: 4\n",
      "Episode 180, Step 30, Reward: -4507.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 181, Step 15, Reward: -2203.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 182, Step 27, Reward: -6404.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 183, Step 81, Reward: -19213.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 184, Step 18, Reward: -3503.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 185, Step 46, Reward: -7810.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 186, Step 22, Reward: -5103.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 187, Step 30, Reward: -6305.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 188, Step 35, Reward: -5908.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 189, Step 65, Reward: -13512.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 190, Step 87, Reward: -20814.00, Evacuated: 0, Deactivated: 4\n",
      "Episode 191, Step 33, Reward: -6706.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 192, Step 35, Reward: -5808.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 193, Step 78, Reward: -21010.20, Evacuated: 0, Deactivated: 4\n",
      "Episode 194, Step 25, Reward: -6403.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 195, Step 19, Reward: -3903.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 196, Step 30, Reward: -6205.80, Evacuated: 0, Deactivated: 4\n",
      "Episode 197, Step 5, Reward: -1500.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 198, Step 29, Reward: -4307.30, Evacuated: 0, Deactivated: 4\n",
      "Episode 199, Step 35, Reward: -8405.60, Evacuated: 0, Deactivated: 4\n",
      "Episode 200, Step 55, Reward: -12309.70, Evacuated: 0, Deactivated: 4\n",
      "Episode 201, Step 25, Reward: -4905.10, Evacuated: 0, Deactivated: 4\n",
      "Episode 202, Step 29, Reward: -3108.50, Evacuated: 0, Deactivated: 4\n",
      "Episode 203, Step 42, Reward: -10406.40, Evacuated: 0, Deactivated: 4\n",
      "Episode 204, Step 23, Reward: -5104.10, Evacuated: 0, Deactivated: 4\n",
      "\n",
      "Simulation interrupted by the user\n"
     ]
    }
   ],
   "source": [
    "# Environment and agent configuration\n",
    "env, agent, config = simulation_config('config.json')\n",
    "max_episodes = config.get('max_episodes')\n",
    "\n",
    "device = agent.device\n",
    "\n",
    "# Metrics to follow the performance\n",
    "all_rewards = []\n",
    "total_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "# Initial reset of the environment\n",
    "#state, info = env.reset()\n",
    "time.sleep(1)\n",
    "try:\n",
    "    while episode_count < max_episodes:\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Determine agents actions\n",
    "            actions = agent.get_action(state)\n",
    "\n",
    "            # Execution of a simulation step\n",
    "            next_state, rewards, terminated, truncated, info = env.step(actions)\n",
    "            total_reward += np.sum(rewards)\n",
    "\n",
    "            agent.replay_buffer.append((state, actions, rewards, next_state, terminated))\n",
    "            # Update agent policy\n",
    "            agent.update_policy()\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        # Update the target network\n",
    "        agent.update_target_networks()\n",
    "        \n",
    "        # Display of the step information\n",
    "        print(f\"\\rEpisode {episode_count + 1}, Step {info['current_step']}, \"\n",
    "                f\"Reward: {total_reward:.2f}, \"\n",
    "                f\"Evacuated: {len(info['evacuated_agents'])}, \"\n",
    "                f\"Deactivated: {len(info['deactivated_agents'])}\", end='')\n",
    "        \n",
    "        # Pause\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # If the episode is terminated\n",
    "        if terminated or truncated:\n",
    "            print(\"\\r\")\n",
    "            episode_count += 1\n",
    "            all_rewards.append(total_reward)\n",
    "            total_reward = 0\n",
    "            \n",
    "            if episode_count < max_episodes:\n",
    "                state, info = env.reset()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nSimulation interrupted by the user\")\n",
    "\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: [ -102.9 -2700.3  -902.1 -2200.8]\n",
      "\n",
      "torch.return_types.max(\n",
      "values=tensor([0.7853, 0.3882, 0.7632, 0.3646, 1.1016, 1.1825, 0.7055, 1.2483, 0.6554,\n",
      "        0.6751, 0.6955, 0.6307, 0.3731, 1.1976, 1.7463, 0.6784, 1.7775, 2.4281,\n",
      "        0.6367, 1.7396, 0.7273, 0.3710, 1.1331, 2.4281, 0.3731, 0.3705, 0.7549,\n",
      "        0.7790, 0.1174, 2.4689, 0.3710, 0.3646], device='cuda:0',\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([3, 5, 3, 5, 5, 5, 3, 5, 3, 3, 3, 3, 5, 5, 5, 3, 5, 5, 3, 5, 3, 5, 5, 5,\n",
      "        5, 5, 3, 3, 2, 5, 5, 5], device='cuda:0'))\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(rewards_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(dones_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 46\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mrewards_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_q_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdones_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Convert actions to indices\u001b[39;00m\n\u001b[0;32m     49\u001b[0m action_indices \u001b[38;5;241m=\u001b[39m [[env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mindex(action) \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m action_list] \u001b[38;5;28;01mfor\u001b[39;00m action_list \u001b[38;5;129;01min\u001b[39;00m actions]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Environment and agent configuration\n",
    "env, agent, config = simulation_config('config.json')\n",
    "max_episodes = config.get('max_episodes')\n",
    "\n",
    "device = agent.device\n",
    "\n",
    "# Metrics to follow the performance\n",
    "all_rewards = []\n",
    "total_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "# Initial reset of the environment\n",
    "state, info = env.reset()\n",
    "time.sleep(1)\n",
    "\n",
    "try:\n",
    "    while episode_count < max_episodes:\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            actions = agent.get_action(state, env.action_space)\n",
    "            next_state, reward, done, truncated, info = env.step(actions)\n",
    "            agent.replay_buffer.append((state, actions, reward, next_state, done))\n",
    "\n",
    "            if len(agent.replay_buffer) > agent.batch_size:\n",
    "                batch = random.sample(agent.replay_buffer, agent.batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                # Move tensors to GPU directly\n",
    "                \n",
    "                states_tensor_list = [torch.tensor(np.array(state), dtype=torch.float32) for state in states]\n",
    "                next_states_tensor_list = [torch.tensor(np.array(next_state), dtype=torch.float32) for next_state in next_states]\n",
    "                states_tensor = torch.stack(states_tensor_list).flatten(start_dim=1).to(device)\n",
    "                rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "                next_states_tensor = torch.stack(next_states_tensor_list).flatten(start_dim=1).to(device)\n",
    "                dones_tensor = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values = agent.ai(states_tensor)\n",
    "                next_q_values = agent.target_network(next_states_tensor)\n",
    "                print(torch.max(next_q_values, dim=1))\n",
    "                print(rewards_tensor.shape)\n",
    "                print(dones_tensor.shape)\n",
    "                target_q_values = rewards_tensor + agent.gamma * torch.max(next_q_values, dim=1)[0] * (1 - dones_tensor)\n",
    "\n",
    "                # Convert actions to indices\n",
    "                action_indices = [[env.action_space.index(action) for action in action_list] for action_list in actions]\n",
    "                actions_tensor = torch.tensor(action_indices, dtype=torch.long, device=agent.device)\n",
    "\n",
    "                selected_q_values = q_values.gather(1, actions_tensor).mean(dim=1)\n",
    "\n",
    "                loss = agent.loss_fn(selected_q_values, target_q_values.detach())\n",
    "                agent.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                agent.optim.step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "        \n",
    "        # Update target network\n",
    "        if episode_count % 10 == 0:\n",
    "            agent.target_network.load_state_dict(agent.ai.state_dict())\n",
    "\n",
    "        if episode_count % 50 == 0:\n",
    "            print(f\"Episode {episode_count}, Reward: {total_reward}\")\n",
    "            \n",
    "        # Pause\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # If the episode is terminated\n",
    "        if done or truncated:\n",
    "            print(\"\\r\")\n",
    "            episode_count += 1\n",
    "            all_rewards.append(total_reward)\n",
    "            total_reward = 0\n",
    "            \n",
    "            if episode_count < max_episodes:\n",
    "                state, info = env.reset()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nSimulation interrupted by the user\")\n",
    "\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate.plot_cumulated_rewards(all_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

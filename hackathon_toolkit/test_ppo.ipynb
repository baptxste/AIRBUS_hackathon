{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from typing import Tuple, Optional, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ModuleNotFoundError:\n",
    "    _GYM_AVAILABLE = False\n",
    "else:\n",
    "    _GYM_AVAILABLE = True\n",
    "\n",
    "from env import MazeEnv\n",
    "from agent_basic import MyAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(input_shape: Tuple[int], n_actions: int, hidden_sizes: list = [128, 128]):\n",
    "    \"\"\"\n",
    "    Simple Multi-Layer Perceptron network\n",
    "    \n",
    "    param input_shape: Shape of input tensor\n",
    "    param n_actions: Number of actions to output\n",
    "    param hidden_sizes: List of sizes of hidden layers\n",
    "    \"\"\"\n",
    "    net_layers = []\n",
    "    \n",
    "    net_layers.append(torch.nn.Linear(input_shape[0],hidden_sizes[0]))\n",
    "    net_layers.append(torch.nn.ReLU())\n",
    "    for i in range(1,len(hidden_sizes)):\n",
    "        net_layers.append(torch.nn.Linear(hidden_sizes[i-1],hidden_sizes[i]))\n",
    "        net_layers.append(torch.nn.ReLU())\n",
    "    net_layers.append(torch.nn.Linear(hidden_sizes[-1],n_actions))\n",
    "    #net_layers.append(torch.nn.Softmax(-1))\n",
    "\n",
    "    \n",
    "    return torch.nn.Sequential(*net_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Actor and Critic networks\n",
    "class DiscreteActor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, actor_net: torch.nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor_net = actor_net\n",
    "\n",
    "    def forward(self, states) -> Tuple[torch.distributions.Categorical, torch.Tensor]:\n",
    "        '''\n",
    "\n",
    "        :param states: state of environment\n",
    "        :return: Probabilities of actions and chosen action\n",
    "        '''\n",
    "        logits = self.actor_net(states)\n",
    "        pi = torch.distributions.Categorical(logits=logits)\n",
    "        action = pi.sample()\n",
    "        return pi, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(object):\n",
    "    \"\"\"\n",
    "    Actor Critic Agent used during trajectory collection. It returns a\n",
    "    distribution and an action given an observation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actor_net: torch.nn.Module, critic_net: torch.nn.Module):\n",
    "        self.actor_net = actor_net\n",
    "        self.critic_net = critic_net\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, state: torch.Tensor) -> Tuple[\n",
    "        torch.distributions.Categorical, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        :param state:\n",
    "        :return: Categorical distribution, action, log probability of action, critic value of state\n",
    "        '''\n",
    "        \n",
    "        # TODO: Calculate the probabilities and the action chosen by the actor as well as the value returned by the critic\n",
    "        pi, _ = self.actor_net(state)\n",
    "\n",
    "        action = pi.sample()\n",
    "        log_p = pi.log_prob(action)\n",
    "        value = self.critic_net(state)\n",
    "        return pi, action, log_p, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(MazeEnv):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: str,\n",
    "            gamma: float = 0.99,\n",
    "            lam: float = 0.95,\n",
    "            lr_actor: float = 1e-3,\n",
    "            lr_critic: float = 1e-3,\n",
    "            max_episode_len: float = 1000,\n",
    "            batch_size: int = 512,\n",
    "            steps_per_epoch: int = 2048,\n",
    "            clip_ratio: float = 0.2,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not _GYM_AVAILABLE:\n",
    "            raise ModuleNotFoundError('This Module requires gym environment which is not installed yet.')\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma  # only needed for trajectory\n",
    "        self.lam = lam  # only needed for trajectory\n",
    "        self.max_episode_len = max_episode_len\n",
    "        self.clip_ratio = clip_ratio  # epsilon\n",
    "        self.automatic_optimization = False\n",
    "        self.save_hyperparameters()\n",
    "        # Read config\n",
    "        with open('config.json', 'r') as config_file:\n",
    "            config = json.load(config_file)\n",
    "\n",
    "        # Env configuration\n",
    "        self.env = MazeEnv(\n",
    "            size=config.get('grid_size'),                               # Grid size\n",
    "            walls_proportion=config.get('walls_proportion'),            # Walls proportion in the grid\n",
    "            num_dynamic_obstacles=config.get('num_dynamic_obstacles'),  # Number of dynamic obstacles\n",
    "            num_agents=config.get('num_agents'),                        # Number of agents\n",
    "            communication_range=config.get('communication_range'),      # Maximum distance for agent communications\n",
    "            max_lidar_dist_main=config.get('max_lidar_dist_main'),      # Maximum distance for main LIDAR scan\n",
    "            max_lidar_dist_second=config.get('max_lidar_dist_second'),  # Maximum distance for secondary LIDAR scan\n",
    "            max_episode_steps=config.get('max_episode_steps'),          # Number of steps before episode termination\n",
    "            render_mode=config.get('render_mode', None),\n",
    "            seed=config.get('seed', None)                               # Seed for reproducibility\n",
    "        )\n",
    "\n",
    "        self.critic = create_mlp(self.env.observation_space.shape, 1)\n",
    "        actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\n",
    "        self.actor = DiscreteActor(actor_mlp)\n",
    "\n",
    "        self.agent = ActorCritic(self.actor, self.critic)\n",
    "\n",
    "        self.batch_states = []\n",
    "        self.batch_actions = []\n",
    "        self.batch_adv = []\n",
    "        self.batch_d_rewards = []\n",
    "        self.batch_logp = []\n",
    "\n",
    "        self.ep_rewards = []\n",
    "        self.ep_values = []\n",
    "        self.epoch_rewards = []\n",
    "\n",
    "        self.episode_step = 0\n",
    "        self.avg_ep_reward = 0\n",
    "        self.avg_ep_len = 0\n",
    "        self.avg_reward = 0\n",
    "\n",
    "        self.state = torch.FloatTensor(self.env.reset()[0])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Passes in a state x through the network and returns the policy and a sampled action\n",
    "        Args:\n",
    "            x: environment state\n",
    "        Returns:\n",
    "            Tuple of policy and action\n",
    "        \"\"\"\n",
    "        action_prob, action = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return action_prob, action, value\n",
    "\n",
    "    def actor_loss(self, state, action, logp_old, advantage) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the actor loss.\n",
    "\n",
    "        Args:\n",
    "            state: current state of environment\n",
    "            action: selected action\n",
    "            logp_old: old log-probability\n",
    "            advantage: advantage of action\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Implement the PPO Actor Loss\n",
    "        pi,_ = self.actor.forward(state)\n",
    "        logpi = pi.log_prob(action)\n",
    "\n",
    "        quotient = torch.exp(logpi)/torch.exp(logp_old)\n",
    "        new_adv = torch.clamp(quotient,1-self.clip_ratio,1+self.clip_ratio)*advantage\n",
    "        f = lambda x: x if x < 1+self.clip_ratio else 1+self.clip_ratio\n",
    "        with torch.no_grad():\n",
    "            quotient.data.apply_(f)\n",
    "\n",
    "        loss_actor = -(quotient*advantage).mean()\n",
    "       \n",
    "        \n",
    "        return loss_actor\n",
    "\n",
    "    def critic_loss(self, state: torch.Tensor, d_reward: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the critic loss.\n",
    "\n",
    "        Args:\n",
    "            state: current state of environment\n",
    "            d_reward: discounted reward\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        # TODO: Implemente the PPO Critic Loss\n",
    "        value = self.agent(state=state)[-1]\n",
    "        loss_critic = ((value - d_reward)**2).mean()\n",
    "        \n",
    "        return loss_critic\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx):\n",
    "        \"\"\"\n",
    "        Carries out a single update to actor and critic network from a batch of replay buffer.\n",
    "\n",
    "        Args:\n",
    "            batch: batch of replay buffer/trajectory data\n",
    "            batch_idx: used for logging\n",
    "            optimizer_idx: idx that controls optimizing actor or critic network\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        optims = self.optimizers()\n",
    "        optim = optims[0] if batch_idx % 2 == 0 else optims[1]\n",
    "        optim.zero_grad()\n",
    "\n",
    "        state, action, old_logp, d_reward, advantage = batch\n",
    "\n",
    "        # normalize advantages\n",
    "        advantage = (advantage - advantage.mean()) / advantage.std()\n",
    "\n",
    "        self.log(\"avg_ep_len\", self.trainer.datamodule.avg_ep_len, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"avg_ep_reward\", self.trainer.datamodule.avg_ep_reward, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"avg_reward\", self.trainer.datamodule.avg_reward, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        if batch_idx % 2 == 0:\n",
    "            loss_actor = self.actor_loss(state, action, old_logp, advantage)\n",
    "            self.log('loss_actor', loss_actor, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "            self.manual_backward(loss_actor)\n",
    "            optim.step()\n",
    "\n",
    "            return loss_actor\n",
    "\n",
    "        elif batch_idx % 2 == 0:\n",
    "            loss_critic = self.critic_loss(state, d_reward)\n",
    "            self.log('loss_critic', loss_critic, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "            self.manual_backward(loss_critic)\n",
    "            optim.step()\n",
    "\n",
    "            return loss_critic\n",
    "\n",
    "    def viz_agent(self):\n",
    "        '''\n",
    "        Visualize actions of the trained agent in environment in a loop, you should use the built in env.render() functionality\n",
    "        :return:\n",
    "        '''\n",
    "        \n",
    "        imgs = []\n",
    "        fig = plt.figure()\n",
    "        state = torch.FloatTensor(self.env.reset()[0])\n",
    "        img = self.env.render()\n",
    "        imgs.append(img)\n",
    "        \n",
    "         # TODO: implement full evaluation loop of environment and use env.render() to get images\n",
    "        state_imgs = []\n",
    "        for _ in range(self.max_episode_len):\n",
    "            _, action, _, _ = self.agent(state) \n",
    "            new_state,_,terminated,*_ = self.env.step(action.cpu().numpy()) \n",
    "            state = torch.FloatTensor(new_state)\n",
    "            if terminated:\n",
    "                break\n",
    "            state_imgs.append(self.env.render())\n",
    "        imgs += state_imgs\n",
    "                 \n",
    "        im = plt.imshow(imgs[0])\n",
    "        print('Episode length', len(imgs))\n",
    "        def animate(i):\n",
    "            im.set_array((imgs[i]))\n",
    "            return [im]\n",
    "        \n",
    "        anim = FuncAnimation(fig, animate, frames=len(imgs), interval=20)\n",
    "        return anim\n",
    "\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\" Initialize Adam optimizer\"\"\"\n",
    "        optimizer_actor = optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "        optimizer_critic = optim.Adam(self.critic.parameters(), lr=self.lr_critic)\n",
    "\n",
    "        return optimizer_actor, optimizer_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vmas in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from vmas) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from vmas) (2.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.27 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from vmas) (1.5.27)\n",
      "Requirement already satisfied: gym in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from vmas) (0.26.2)\n",
      "Requirement already satisfied: six in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from vmas) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym->vmas) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym->vmas) (0.0.8)\n",
      "Requirement already satisfied: filelock in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch->vmas) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch->vmas) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch->vmas) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch->vmas) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch->vmas) (2024.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch->vmas) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy==1.13.1->torch->vmas) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\etien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch->vmas) (2.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\etien\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install vmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
    "n_iters = 10  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100  # Episode steps before done\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized envs. frames_per_batch should be divisible by this number\n",
    "scenario_name = \"navigation\"\n",
    "n_agents = 3\n",
    "\n",
    "env = VmasEnv(\n",
    "    scenario=scenario_name,\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,  # VMAS supports both continuous and discrete actions\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    # Scenario kwargs\n",
    "    n_agents=n_agents,  # These are custom kwargs that change for each VMAS scenario, see the VMAS repo to know more.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etien\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchrl\\envs\\common.py:1105: DeprecationWarning: You are querying a non-trivial, single action_spec, i.e., there is only one action known by the environment but it is not named `'action'`. Currently, env.action_spec returns the leaf but for consistency with the setter, this will return the full spec instead (from v0.8 and on).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "share_parameters_policy = True\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=2 * env.action_spec.shape[-1],  # 2 * n_actions_per_agents\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1,  # 1 value per agent\n",
    "    n_agents=env.n_agents,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    num_cells=256,\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"state_value\")],\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 10:13:00,702 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 5, 3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([60, 5, 3]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agents: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        info: TensorDict(\n",
      "                            fields={\n",
      "                                agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                            batch_size=torch.Size([60, 5, 3]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([60, 5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([60, 5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([60, 5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etien\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchrl\\envs\\libs\\vmas.py:374: UserWarning: unbatched_action_spec is deprecated and will be removed in v0.9. Please use full_action_spec_unbatched instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.unbatched_action_spec,\n",
    "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "        \"high\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")  # we'll need the log-prob for the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  # We store the frames_per_batch collected at each iteration\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    ")\n",
    "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "    value=(\"agents\", \"state_value\"),\n",
    "    # These last 2 keys will be expanded to match the reward shape\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")  # We build GAE\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = 2.9131367206573486: 100%|██████████| 10/10 [01:33<00:00,  8.88s/it] "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for tensordict_data in collector:\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        GAE(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_reward_mean_list)\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode reward mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Multi agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from typing import Tuple, Optional, Dict\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from env import MazeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    '''Simple buffer to collect experiences and clear after each update.'''\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.state_values = []\n",
    "    \n",
    "    def clear_buffer(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.dones[:]\n",
    "        del self.state_values[:]\n",
    "    \n",
    "    def get_ordered_trajectories(self, n_agents=None):\n",
    "        ordered_actions = torch.FloatTensor()\n",
    "        ordered_states = torch.FloatTensor()\n",
    "        ordered_logprobs = torch.FloatTensor()\n",
    "        ordered_rewards = []\n",
    "        ordered_dones = []\n",
    "        \n",
    "        actions = torch.stack(self.actions)\n",
    "        states = torch.stack(self.states)\n",
    "        logprobs = torch.stack(self.logprobs)\n",
    "\n",
    "        self.ordered_actions = torch.FloatTensor()\n",
    "        for index in range(actions.shape[1]):\n",
    "            if n_agents !=None and n_agents == index+1:\n",
    "                break\n",
    "            ordered_states = torch.cat((ordered_states, states[:, index]), 0)\n",
    "            ordered_actions = torch.cat((ordered_actions, actions[:, index]), 0)\n",
    "            ordered_logprobs = torch.cat((ordered_logprobs, logprobs[:, index]), 0)\n",
    "            ordered_rewards.extend(np.asarray(self.rewards)[:, index])\n",
    "            ordered_dones.extend(np.asarray(self.dones)[:, index])\n",
    "\n",
    "        return ordered_states, ordered_actions, ordered_logprobs, ordered_rewards, ordered_dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, action_std=0.5, hidden_size=32, low_policy_weights_init=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor_fc1 = nn.Linear(state_size, 2*hidden_size)\n",
    "        self.actor_fc2 = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.actor_fc3 = nn.Linear(2*hidden_size, hidden_size)\n",
    "\n",
    "        self.actor_mu = nn.Linear(hidden_size, action_size)\n",
    "        self.actor_sigma = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        \n",
    "        self.critic_fc1 = nn.Linear(state_size, 2*hidden_size)\n",
    "        self.critic_fc2 = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.critic_fc3 = nn.Linear(2*hidden_size, hidden_size)\n",
    "\n",
    "        self.critic_value = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.distribution = torch.distributions.Normal\n",
    "\n",
    "        self.action_var = torch.full((action_size,), action_std*action_std)\n",
    "        \n",
    "        # Boosts training performance in the beginning\n",
    "        if low_policy_weights_init:\n",
    "            with torch.no_grad():\n",
    "                self.actor_mu.weight.mul_(0.01)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.tanh(self.actor_fc1(state))\n",
    "        x = torch.tanh(self.actor_fc2(x))\n",
    "        x = torch.tanh(self.actor_fc3(x))\n",
    "        mu = torch.tanh(self.actor_mu(x))\n",
    "        sigma = F.softplus(self.actor_sigma(x))\n",
    "\n",
    "        v = torch.tanh(self.critic_fc1(state))\n",
    "        v = torch.tanh(self.critic_fc2(v))\n",
    "        v = torch.tanh(self.critic_fc3(v))\n",
    "        state_value = self.critic_value(v)\n",
    "\n",
    "        return mu, sigma, state_value \n",
    "\n",
    "    def act(self, state):\n",
    "        '''Choose action according to the policy.'''\n",
    "        action_mu, action_sigma, state_value = self.forward(state)\n",
    "\n",
    "        action_var = self.action_var.expand_as(action_mu)\n",
    "        cov_mat = torch.diag_embed(action_var)\n",
    "        dist = MultivariateNormal(action_mu, cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach(), log_prob.detach()\n",
    "    \n",
    "    def evaluateStd(self, state, action):\n",
    "        '''Evaluate action using learned std value for distribution.'''\n",
    "        action_mu, action_sigma, state_value = self.forward(state)\n",
    "        m = self.distribution(action_mu.squeeze(), action_sigma.squeeze())\n",
    "        log_prob = m.log_prob(action)\n",
    "\n",
    "        return log_prob, state_value\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        '''Evaluate action for a given state.'''   \n",
    "        action_mean, _, state_value = self.forward(state)\n",
    "        \n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var)\n",
    "        \n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    '''Proximal Policy Optimization algorithm.'''\n",
    "    def __init__(self, state_size, action_size, lr=1e-4, gamma=0.99, epsilon_clip=0.2, epochs=20, action_std=0.5):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma  = gamma\n",
    "        self.epsilon_clip = epsilon_clip\n",
    "        self.K_epochs = epochs\n",
    "\n",
    "        self.policy = ActorCritic(self.state_size, self.action_size, action_std, hidden_size=128)\n",
    "        self.policy_old = ActorCritic(self.state_size, self.action_size, action_std, hidden_size=128)\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr, betas=(0.9, 0.999))\n",
    "\n",
    "        self.episode = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        '''Get action using state in numpy format'''\n",
    "        # state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        state = torch.FloatTensor(state)\n",
    "        \n",
    "        return self.policy_old.act(state)\n",
    "\n",
    "    def update(self, memory):\n",
    "        '''Update agent's network using collected set of experiences.'''\n",
    "        states, actions, log_probs, rewards, dones = memory.get_ordered_trajectories(5)\n",
    "\n",
    "        discounted_rewards = []\n",
    "        discounted_reward = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if dones[i] == True:\n",
    "                discounted_reward = 0  \n",
    "            discounted_reward = rewards[i] + self.gamma*discounted_reward\n",
    "            discounted_rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        # old_state_values = torch.stack(state_values, 1).detach()\n",
    "        # advantages = discounted_rewards - old_state_values.detach().squeeze()\n",
    "        # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
    "        \n",
    "        # states = torch.squeeze(torch.stack(states), 1).detach()\n",
    "        # actions = torch.squeeze(torch.stack(actions), 1).detach()\n",
    "        # old_log_probs = torch.squeeze(torch.stack(log_probs), 1).detach()\n",
    "\n",
    "        states = states.detach()\n",
    "        actions = actions.detach()\n",
    "        old_log_probs = log_probs.detach()\n",
    "\n",
    "\n",
    "        for epoch in range(self.K_epochs):\n",
    "\n",
    "            new_log_probs, state_values, dist_entropy = self.policy.evaluate(states, actions)\n",
    "\n",
    "            new_log_probs = new_log_probs.squeeze()\n",
    "            advantages = discounted_rewards - state_values.detach().squeeze()\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            ratios_clipped = torch.clamp(ratios, min=1-self.epsilon_clip, max=1+self.epsilon_clip)\n",
    "            loss = -torch.min(ratios*advantages, ratios_clipped*advantages)+ 0.5*self.MseLoss(state_values, discounted_rewards) - 0.01*dist_entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_config(config_path: str, new_agent: bool = True):\n",
    "    \"\"\"\n",
    "    Configure the environment and optionally an agent using a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the configuration JSON file.\n",
    "        new_agent (bool): Whether to initialize the agent. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[MazeEnv, Optional[MyAgent], Dict]: Configured environment, agent (if new), and the configuration dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read config\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Env configuration\n",
    "    env = MazeEnv(\n",
    "        size=config.get('grid_size'),                               # Grid size\n",
    "        walls_proportion=config.get('walls_proportion'),            # Walls proportion in the grid\n",
    "        num_dynamic_obstacles=config.get('num_dynamic_obstacles'),  # Number of dynamic obstacles\n",
    "        num_agents=config.get('num_agents'),                        # Number of agents\n",
    "        communication_range=config.get('communication_range'),      # Maximum distance for agent communications\n",
    "        max_lidar_dist_main=config.get('max_lidar_dist_main'),      # Maximum distance for main LIDAR scan\n",
    "        max_lidar_dist_second=config.get('max_lidar_dist_second'),  # Maximum distance for secondary LIDAR scan\n",
    "        max_episode_steps=config.get('max_episode_steps'),          # Number of steps before episode termination\n",
    "        render_mode=config.get('render_mode', None),\n",
    "        seed=config.get('seed', None)                               # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Agent configuration\n",
    "    agents = PPO(state_size=env.single_agent_state_size*env.num_agents,action_size=env.num_agents * env.action_space.n) if new_agent else None\n",
    "\n",
    "    return env, agents, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n",
      "1 [ 1 14] [array([-1, 14]), array([-1, 14])]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x22 and 44x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m time_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     55\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(states)\u001b[38;5;241m.\u001b[39mview(n_agents, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m actions, log_probs \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mview(n_agents, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     58\u001b[0m pi \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mactions)\n",
      "Cell \u001b[1;32mIn[25], line 25\u001b[0m, in \u001b[0;36mPPO.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# state = torch.FloatTensor(state.reshape(1, -1))\u001b[39;00m\n\u001b[0;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_old\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 44\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Choose action according to the policy.'''\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     action_mu, action_sigma, state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     action_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_var\u001b[38;5;241m.\u001b[39mexpand_as(action_mu)\n\u001b[0;32m     47\u001b[0m     cov_mat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag_embed(action_var)\n",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_fc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_fc2(x))\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_fc3(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x22 and 44x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "n_agents = 2\n",
    "n_episodes = 4000\n",
    "max_steps = 1600\n",
    "update_interval = 16000/n_agents\n",
    "log_interval = 10\n",
    "solving_threshold = 30\n",
    "time_step = 0\n",
    "\n",
    "render = False\n",
    "train = True\n",
    "pretrained = False\n",
    "tensorboard_logging = True\n",
    "\n",
    "env, agent, config = simulation_config('config.json', new_agent=True)\n",
    "\n",
    "scores = deque(maxlen=log_interval)\n",
    "max_score = -1000\n",
    "episode_lengths = deque(maxlen=log_interval)\n",
    "rewards =  []\n",
    "\n",
    "memory = MemoryBuffer()\n",
    "\n",
    "if not train:\n",
    "    agent.policy_old.eval()\n",
    "else:\n",
    "    writer = SummaryWriter(log_dir='logs/'+str(time.time()))\n",
    "\n",
    "if pretrained:\n",
    "    agent.policy_old.load_state_dict(torch.load('./_model_best_old.pth'))\n",
    "    agent.policy.load_state_dict(torch.load('./_model_best_old.pth'))\n",
    "\n",
    "\n",
    "for n_episode in range(1, n_episodes+1):\n",
    "    states, env_info = env.reset()\n",
    "    state = states[0]\n",
    "    states = torch.FloatTensor(states).view(1, -1)\n",
    "    # print(\"States shape: \", states.shape)\n",
    "    # state = torch.FloatTensor(state.reshape(1, -1))\n",
    "    episode_length = 0\n",
    "    episodic_rewards = []\n",
    "    for t in range(max_steps):\n",
    "        time_step += 1\n",
    "        states = torch.FloatTensor(states).flatten() \n",
    "        actions, log_probs = agent.select_action(states)\n",
    "        pi = torch.distributions.Categorical(logits=actions.view(n_agents, -1))\n",
    "        log_probs = pi.log_prob(torch.tensor(actions))\n",
    "        prob_agent = actions.view(n_agents, -1)\n",
    "        \n",
    "        actions_step = [0]*n_agents\n",
    "        for i in range(n_agents):\n",
    "            pi = torch.distributions.Categorical(logits=prob_agent[i])\n",
    "            actions_step[i] = pi.sample().item()\n",
    "        memory.actions.append(actions)\n",
    "        memory.logprobs.append(log_probs)\n",
    "        memory.states.append(states)\n",
    "        # echantillonage des actions\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # actions = []\n",
    "        # ## Unity env style\n",
    "        # for agent_id in range(0,20):\n",
    "        #     actions.append(action.data.numpy().flatten())\n",
    "\n",
    "        states, rewards, terminated, truncated, info = env.step(actions_step)         # send all actions to tne environment\n",
    "        \n",
    "        # states = env_info.vector_observations         # get next state (for each agent)\n",
    "        # rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        # dones = env_info.local_done   \n",
    "        dones = [terminated]*env.num_agents\n",
    "        state = states[0]\n",
    "        reward = rewards[0]\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # state, reward, done, _ = env.step(action.data.numpy().flatten())\n",
    "\n",
    "\n",
    "        memory.rewards.append(rewards)\n",
    "        memory.dones.append(dones)\n",
    "        episodic_rewards.append(rewards)\n",
    "        state_value = 0\n",
    "        \n",
    "        # if render:\n",
    "        #     image = env.render(mode = 'rgb_array')\n",
    "            # if time_step % 2 == 0:\n",
    "            #     writerImage.append_data(image)\n",
    "\n",
    "        if train:\n",
    "            if time_step % update_interval == 0:\n",
    "                agent.update(memory)\n",
    "                time_step = 0\n",
    "                memory.clear_buffer()\n",
    "\n",
    "        episode_length = t\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_lengths.append(episode_length)\n",
    "    total_reward = np.sum(episodic_rewards)/n_agents\n",
    "    scores.append(total_reward)\n",
    "    \n",
    "    if train:\n",
    "        if n_episode % log_interval == 0:\n",
    "            print(\"Episode: \", n_episode, \"\\t Avg. episode length: \", np.mean(episode_lengths), \"\\t Avg. score: \", np.mean(scores))\n",
    "\n",
    "            if np.mean(scores) > solving_threshold:\n",
    "                print(\"Environment solved, saving model\")\n",
    "                torch.save(agent.policy_old.state_dict(), 'PPO_model_solved_{}.pth'.format(time.time()))\n",
    "        \n",
    "        if n_episode % 100 == 0:\n",
    "            print(\"Saving model after \", n_episode, \" episodes\")\n",
    "            torch.save(agent.policy_old.state_dict(), '{}_model_{}_episodes.pth'.format(time.time(), n_episode))\n",
    "            \n",
    "        if total_reward > max_score:\n",
    "            print(\"Saving improved model\")\n",
    "            max_score = total_reward\n",
    "            torch.save(agent.policy_old.state_dict(), '{}_model_best.pth'.format(time.time()))\n",
    "\n",
    "        if tensorboard_logging:\n",
    "            writer.add_scalars('Score', {'Score':total_reward, 'Avg._Score': np.mean(scores)}, n_episode)\n",
    "            writer.add_scalars('Episode_length', {'Episode_length':episode_length, 'Avg._Episode length': np.mean(episode_lengths)}, n_episode)\n",
    "    \n",
    "    else:\n",
    "        print(\"Episode: \", n_episode, \"\\t Episode length: \", episode_length, \"\\t Score: \", total_reward)\n",
    "        \n",
    "    total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x4 and 2x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env, model, num_episodes)\u001b[39m\n\u001b[32m     71\u001b[39m edge_index = create_edge_index(num_agents)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Calculer les actions (déplacements) avec le modèle GNN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m actions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Faire un pas dans l'environnement\u001b[39;00m\n\u001b[32m     77\u001b[39m new_positions, new_velocities, reward = env.step(actions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mSimpleGNN.forward\u001b[39m\u001b[34m(self, x, edge_index)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     x = torch.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv2(x, edge_index)\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:260\u001b[39m, in \u001b[36mGCNConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_weight)\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m             edge_index = cache\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[32m    263\u001b[39m out = \u001b[38;5;28mself\u001b[39m.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/hack/lib/python3.12/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    142\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[32m    143\u001b[39m \n\u001b[32m    144\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (3x4 and 2x4)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import random\n",
    "\n",
    "# Définir un environnement simple avec des agents\n",
    "class SimpleEnv:\n",
    "    def __init__(self, num_agents):\n",
    "        self.num_agents = num_agents\n",
    "        self.positions = torch.rand((num_agents, 2))  # Positions initiales aléatoires des agents\n",
    "        self.velocities = torch.rand((num_agents, 2))  # Vitesse initiale aléatoire des agents\n",
    "        self.time_step = 0.1  # Pas de temps pour la simulation\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = torch.rand((self.num_agents, 2))  # Réinitialiser les positions\n",
    "        self.velocities = torch.rand((self.num_agents, 2))  # Réinitialiser les vitesses\n",
    "        return self.positions, self.velocities\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Mettre à jour les positions et vitesses des agents selon leurs actions\n",
    "        self.velocities += actions * self.time_step  # actions: déplacement des agents\n",
    "        self.positions += self.velocities * self.time_step  # Mettre à jour les positions\n",
    "\n",
    "        # Calculer la récompense en fonction de la proximité des agents\n",
    "        reward = self.compute_reward()\n",
    "        return self.positions, self.velocities, reward\n",
    "\n",
    "    def compute_reward(self):\n",
    "        reward = 0\n",
    "        for i in range(self.num_agents):\n",
    "            for j in range(i+1, self.num_agents):\n",
    "                distance = torch.norm(self.positions[i] - self.positions[j])\n",
    "                reward -= distance  # Plus les agents sont proches, plus la récompense est élevée\n",
    "        return reward\n",
    "\n",
    "# Définir le modèle GNN\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialisation de l'environnement\n",
    "num_agents = 3\n",
    "env = SimpleEnv(num_agents)\n",
    "\n",
    "# Initialisation du modèle GNN\n",
    "input_dim = 2  # Nombre de caractéristiques (position, vitesse)\n",
    "hidden_dim = 4\n",
    "output_dim = 2  # Actions (déplacements des agents)\n",
    "model = SimpleGNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Fonction d'entraînement du modèle\n",
    "def train(env, model, num_episodes=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        positions, velocities = env.reset()\n",
    "\n",
    "        # Préparer les caractéristiques des nœuds (agents)\n",
    "        x = torch.cat((positions, velocities), dim=1)  # Concatenate positions et vitesses\n",
    "\n",
    "        # Créer un graphe (arêtes entre agents proches)\n",
    "        edge_index = create_edge_index(num_agents)\n",
    "\n",
    "        # Calculer les actions (déplacements) avec le modèle GNN\n",
    "        actions = model(x, edge_index)\n",
    "\n",
    "        # Faire un pas dans l'environnement\n",
    "        new_positions, new_velocities, reward = env.step(actions)\n",
    "\n",
    "        # Calculer la perte (nous voulons maximiser la récompense)\n",
    "        loss = -reward  # Le but est de maximiser la récompense, donc perte négative\n",
    "\n",
    "        # Rétropropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {reward.item()}\")\n",
    "\n",
    "# Créer un graphe basé sur la proximité des agents\n",
    "def create_edge_index(num_agents):\n",
    "    edge_index = []\n",
    "    for i in range(num_agents):\n",
    "        for j in range(i+1, num_agents):\n",
    "            edge_index.append([i, j])\n",
    "            edge_index.append([j, i])  # Ajouter les deux directions (i -> j et j -> i)\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Entraîner le modèle\n",
    "train(env, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -1.6426904201507568\n",
      "Episode 10, Reward: -1.1354026794433594\n",
      "Episode 20, Reward: -0.9518758058547974\n",
      "Episode 30, Reward: -1.6432170867919922\n",
      "Episode 40, Reward: -1.60065758228302\n",
      "Episode 50, Reward: -2.17828631401062\n",
      "Episode 60, Reward: -1.8209532499313354\n",
      "Episode 70, Reward: -1.0061274766921997\n",
      "Episode 80, Reward: -1.5181645154953003\n",
      "Episode 90, Reward: -1.7908287048339844\n",
      "Episode 100, Reward: -1.4913365840911865\n",
      "Episode 110, Reward: -1.433598279953003\n",
      "Episode 120, Reward: -1.876486897468567\n",
      "Episode 130, Reward: -1.562821626663208\n",
      "Episode 140, Reward: -1.9320898056030273\n",
      "Episode 150, Reward: -1.3887519836425781\n",
      "Episode 160, Reward: -0.6437910795211792\n",
      "Episode 170, Reward: -1.7163625955581665\n",
      "Episode 180, Reward: -1.241245985031128\n",
      "Episode 190, Reward: -1.543684959411621\n",
      "Episode 200, Reward: -1.1578516960144043\n",
      "Episode 210, Reward: -0.32796913385391235\n",
      "Episode 220, Reward: -1.3898110389709473\n",
      "Episode 230, Reward: -1.0210245847702026\n",
      "Episode 240, Reward: -1.2952320575714111\n",
      "Episode 250, Reward: -1.09363853931427\n",
      "Episode 260, Reward: -1.7767667770385742\n",
      "Episode 270, Reward: -2.573820114135742\n",
      "Episode 280, Reward: -1.6907861232757568\n",
      "Episode 290, Reward: -0.8501242399215698\n",
      "Episode 300, Reward: -1.6022628545761108\n",
      "Episode 310, Reward: -1.613327980041504\n",
      "Episode 320, Reward: -2.402158260345459\n",
      "Episode 330, Reward: -1.3397762775421143\n",
      "Episode 340, Reward: -1.965540885925293\n",
      "Episode 350, Reward: -1.8073880672454834\n",
      "Episode 360, Reward: -1.2881629467010498\n",
      "Episode 370, Reward: -2.0831408500671387\n",
      "Episode 380, Reward: -1.485377550125122\n",
      "Episode 390, Reward: -1.2285490036010742\n",
      "Episode 400, Reward: -2.590928792953491\n",
      "Episode 410, Reward: -1.6487338542938232\n",
      "Episode 420, Reward: -1.8097710609436035\n",
      "Episode 430, Reward: -1.568037509918213\n",
      "Episode 440, Reward: -1.4777673482894897\n",
      "Episode 450, Reward: -2.0971744060516357\n",
      "Episode 460, Reward: -0.9524726867675781\n",
      "Episode 470, Reward: -1.6502978801727295\n",
      "Episode 480, Reward: -1.6696311235427856\n",
      "Episode 490, Reward: -1.9357253313064575\n",
      "Episode 500, Reward: -1.2870314121246338\n",
      "Episode 510, Reward: -1.4107167720794678\n",
      "Episode 520, Reward: -0.888604462146759\n",
      "Episode 530, Reward: -1.556812047958374\n",
      "Episode 540, Reward: -1.9745194911956787\n",
      "Episode 550, Reward: -1.5055657625198364\n",
      "Episode 560, Reward: -1.238813042640686\n",
      "Episode 570, Reward: -1.2478721141815186\n",
      "Episode 580, Reward: -2.0471065044403076\n",
      "Episode 590, Reward: -1.6767542362213135\n",
      "Episode 600, Reward: -1.3415160179138184\n",
      "Episode 610, Reward: -1.5583126544952393\n",
      "Episode 620, Reward: -1.1833593845367432\n",
      "Episode 630, Reward: -1.300543189048767\n",
      "Episode 640, Reward: -1.6382911205291748\n",
      "Episode 650, Reward: -1.3504104614257812\n",
      "Episode 660, Reward: -1.948016881942749\n",
      "Episode 670, Reward: -1.5621733665466309\n",
      "Episode 680, Reward: -1.5102910995483398\n",
      "Episode 690, Reward: -1.0614498853683472\n",
      "Episode 700, Reward: -1.5567805767059326\n",
      "Episode 710, Reward: -0.9256154894828796\n",
      "Episode 720, Reward: -2.264826536178589\n",
      "Episode 730, Reward: -1.1928240060806274\n",
      "Episode 740, Reward: -1.063060998916626\n",
      "Episode 750, Reward: -1.2632020711898804\n",
      "Episode 760, Reward: -1.625152826309204\n",
      "Episode 770, Reward: -1.1402101516723633\n",
      "Episode 780, Reward: -1.5995384454727173\n",
      "Episode 790, Reward: -1.255707025527954\n",
      "Episode 800, Reward: -2.4212889671325684\n",
      "Episode 810, Reward: -1.8317735195159912\n",
      "Episode 820, Reward: -1.795569658279419\n",
      "Episode 830, Reward: -2.353851795196533\n",
      "Episode 840, Reward: -1.0374258756637573\n",
      "Episode 850, Reward: -2.0407280921936035\n",
      "Episode 860, Reward: -1.6752963066101074\n",
      "Episode 870, Reward: -1.1154725551605225\n",
      "Episode 880, Reward: -1.9299144744873047\n",
      "Episode 890, Reward: -1.6101670265197754\n",
      "Episode 900, Reward: -1.620814323425293\n",
      "Episode 910, Reward: -0.9239025115966797\n",
      "Episode 920, Reward: -1.2201611995697021\n",
      "Episode 930, Reward: -1.269889235496521\n",
      "Episode 940, Reward: -2.256401777267456\n",
      "Episode 950, Reward: -1.4587535858154297\n",
      "Episode 960, Reward: -1.588617205619812\n",
      "Episode 970, Reward: -1.547767162322998\n",
      "Episode 980, Reward: -1.016459345817566\n",
      "Episode 990, Reward: -1.4691801071166992\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import random\n",
    "\n",
    "# Définir un environnement simple avec des agents\n",
    "class SimpleEnv:\n",
    "    def __init__(self, num_agents):\n",
    "        self.num_agents = num_agents\n",
    "        self.positions = torch.rand((num_agents, 2))  # Positions initiales aléatoires des agents\n",
    "        self.velocities = torch.rand((num_agents, 2))  # Vitesse initiale aléatoire des agents\n",
    "        self.time_step = 0.1  # Pas de temps pour la simulation\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = torch.rand((self.num_agents, 2))  # Réinitialiser les positions\n",
    "        self.velocities = torch.rand((self.num_agents, 2))  # Réinitialiser les vitesses\n",
    "        return self.positions, self.velocities\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Mettre à jour les positions et vitesses des agents selon leurs actions\n",
    "        self.velocities += actions * self.time_step  # actions: déplacement des agents\n",
    "        self.positions += self.velocities * self.time_step  # Mettre à jour les positions\n",
    "\n",
    "        # Calculer la récompense en fonction de la proximité des agents\n",
    "        reward = self.compute_reward()\n",
    "        return self.positions, self.velocities, reward\n",
    "\n",
    "    def compute_reward(self):\n",
    "        reward = 0\n",
    "        for i in range(self.num_agents):\n",
    "            for j in range(i+1, self.num_agents):\n",
    "                distance = torch.norm(self.positions[i] - self.positions[j])\n",
    "                reward -= distance  # Plus les agents sont proches, plus la récompense est élevée\n",
    "        return reward\n",
    "\n",
    "# Définir le modèle GNN\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))  # Appliquer la première convolution\n",
    "        x = self.conv2(x, edge_index)  # Appliquer la deuxième convolution\n",
    "        return x\n",
    "\n",
    "# Initialisation de l'environnement\n",
    "num_agents = 3\n",
    "env = SimpleEnv(num_agents)\n",
    "\n",
    "# Initialisation du modèle GNN\n",
    "input_dim = 4  # Nombre de caractéristiques (position, vitesse) concaténées\n",
    "hidden_dim = 4\n",
    "output_dim = 2  # Actions (déplacements des agents)\n",
    "model = SimpleGNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Fonction d'entraînement du modèle\n",
    "def train(env, model, num_episodes=1000):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        positions, velocities = env.reset()\n",
    "\n",
    "        # Préparer les caractéristiques des nœuds (agents)\n",
    "        x = torch.cat((positions, velocities), dim=1)  # Concatenate positions et vitesses (4 caractéristiques)\n",
    "\n",
    "        # Créer un graphe (arêtes entre agents proches)\n",
    "        edge_index = create_edge_index(num_agents)\n",
    "\n",
    "        # Calculer les actions (déplacements) avec le modèle GNN\n",
    "        actions = model(x, edge_index)\n",
    "\n",
    "        # Faire un pas dans l'environnement\n",
    "        new_positions, new_velocities, reward = env.step(actions)\n",
    "\n",
    "        # Calculer la perte (nous voulons maximiser la récompense)\n",
    "        loss = -reward  # Le but est de maximiser la récompense, donc perte négative\n",
    "\n",
    "        # Rétropropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {reward.item()}\")\n",
    "\n",
    "# Créer un graphe basé sur la proximité des agents\n",
    "def create_edge_index(num_agents):\n",
    "    edge_index = []\n",
    "    for i in range(num_agents):\n",
    "        for j in range(i+1, num_agents):\n",
    "            edge_index.append([i, j])\n",
    "            edge_index.append([j, i])  # Ajouter les deux directions (i -> j et j -> i)\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Entraîner le modèle\n",
    "train(env, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
